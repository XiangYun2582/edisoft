{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff81ba2",
   "metadata": {},
   "source": [
    "## 網路爬蟲"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e96121",
   "metadata": {},
   "source": [
    "### 抓標題(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7352d308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thread-7 (fetch_url)] 成功抓取：https://httpbin.org/html\n",
      "📝 https://httpbin.org/html 的標題：無標題\n",
      "[Thread-10 (fetch_url)] 成功抓取：https://httpbin.org/user-agent\n",
      "🧾 https://httpbin.org/user-agent 的內容：{\n",
      "  \"user-agent\": \"python-requests/2.32.4\"\n",
      "}\n",
      "...\n",
      "[Thread-9 (fetch_url)] 成功抓取：https://httpbin.org/ip\n",
      "🧾 https://httpbin.org/ip 的內容：{\n",
      "  \"origin\": \"211.20.1.45\"\n",
      "}\n",
      "...\n",
      "[Thread-8 (fetch_url)] 成功抓取：https://httpbin.org/headers\n",
      "🧾 https://httpbin.org/headers 的內容：{\n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept...\n",
      "[Thread-11 (fetch_url)] 成功抓取：https://httpbin.org/get\n",
      "🧾 https://httpbin.org/get 的內容：{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*...\n",
      "✅ 所有任務完成！\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 測試網址（httpbin.org 是常見的測試網站）\n",
    "urls = [\n",
    "    \"https://httpbin.org/html\",\n",
    "    \"https://httpbin.org/headers\",\n",
    "    \"https://httpbin.org/ip\",\n",
    "    \"https://httpbin.org/user-agent\",\n",
    "    \"https://httpbin.org/get\"\n",
    "]\n",
    "\n",
    "# 工作函式：抓取網頁標題或內容\n",
    "def fetch_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        print(f\"[{threading.current_thread().name}] 成功抓取：{url}\")\n",
    "        if \"html\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            title = soup.title.string if soup.title else \"無標題\"\n",
    "            print(f\"📝 {url} 的標題：{title}\")\n",
    "        else:\n",
    "            print(f\"🧾 {url} 的內容：{response.text[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{threading.current_thread().name}] ⚠️ 抓取失敗：{url}\\n原因：{e}\")\n",
    "\n",
    "def multi_thread_crawler(url_list):\n",
    "    threads = []\n",
    "\n",
    "    for url in url_list:\n",
    "        t = threading.Thread(target=fetch_url, args=(url,))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # 等待所有執行緒結束\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    print(\"✅ 所有任務完成！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    multi_thread_crawler(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a5802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 爬蟲結果：\n",
      "https://httpbin.org/headers => 無標題\n",
      "https://httpbin.org/html => 無標題\n",
      "https://httpbin.org/user-agent => 無標題\n",
      "https://httpbin.org/ip => 無標題\n",
      "https://httpbin.org/get => 無標題\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue\n",
    "\n",
    "# 網頁列表（可替換為你要爬的網站）\n",
    "urls = [\n",
    "    \"https://httpbin.org/html\",\n",
    "    \"https://httpbin.org/user-agent\",\n",
    "    \"https://httpbin.org/headers\",\n",
    "    \"https://httpbin.org/ip\",\n",
    "    \"https://httpbin.org/get\"\n",
    "]\n",
    "\n",
    "# 存放爬取結果\n",
    "results = []\n",
    "\n",
    "# 建立佇列\n",
    "url_queue = Queue()\n",
    "\n",
    "# 放入網址\n",
    "for url in urls:\n",
    "    url_queue.put(url)\n",
    "\n",
    "# Lock 來保護共享變數（如 results）\n",
    "lock = threading.Lock()\n",
    "\n",
    "# 爬蟲執行緒函式\n",
    "def crawler_worker():\n",
    "    while not url_queue.empty():\n",
    "        url = url_queue.get()\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            title = soup.title.string.strip() if soup.title else \"無標題\"\n",
    "        except Exception as e:\n",
    "            title = f\"抓取失敗: {e}\"\n",
    "\n",
    "        with lock:\n",
    "            results.append((url, title))\n",
    "\n",
    "        url_queue.task_done()\n",
    "\n",
    "# 建立多個執行緒\n",
    "threads = []\n",
    "for i in range(3):  # 可依需求調整執行緒數\n",
    "    t = threading.Thread(target=crawler_worker, name=f\"Worker-{i+1}\")\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# 使用 join 等待所有執行緒完成\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "# 顯示結果\n",
    "print(\"\\n📋 爬蟲結果：\")\n",
    "for url, title in results:\n",
    "    print(f\"{url} => {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc9260",
   "metadata": {},
   "source": [
    "### 抓標題 (lock 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbf9795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thread-12 (fetch_url)] 正在抓取: https://httpbin.org/html\n",
      "[Thread-13 (fetch_url)] 正在抓取: https://httpbin.org/user-agent\n",
      "[Thread-12 (fetch_url)] ✅ 成功抓取 https://httpbin.org/html -> 無標題\n",
      "[Thread-13 (fetch_url)] ✅ 成功抓取 https://httpbin.org/user-agent -> 無標題\n",
      "[Thread-14 (fetch_url)] 正在抓取: https://httpbin.org/headers\n",
      "[Thread-15 (fetch_url)] 正在抓取: https://httpbin.org/ip\n",
      "[Thread-15 (fetch_url)] ✅ 成功抓取 https://httpbin.org/ip -> 無標題\n",
      "[Thread-14 (fetch_url)] ✅ 成功抓取 https://httpbin.org/headers -> 無標題\n",
      "[Thread-16 (fetch_url)] 正在抓取: https://httpbin.org/get\n",
      "[Thread-16 (fetch_url)] ✅ 成功抓取 https://httpbin.org/get -> 無標題\n",
      "🎉 所有抓取完成\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# 建立 Semaphore，最多允許兩個執行緒同時進入\n",
    "sema = threading.Semaphore(2)\n",
    "\n",
    "urls = [\n",
    "    \"https://httpbin.org/html\",\n",
    "    \"https://httpbin.org/user-agent\",\n",
    "    \"https://httpbin.org/headers\",\n",
    "    \"https://httpbin.org/ip\",\n",
    "    \"https://httpbin.org/get\"\n",
    "]\n",
    "\n",
    "def fetch_url(url):\n",
    "    with sema:  # 取得通行證（同時最多兩個執行緒通過）\n",
    "        print(f\"[{threading.current_thread().name}] 正在抓取: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            title = soup.title.string.strip() if soup.title else \"無標題\"\n",
    "            print(f\"[{threading.current_thread().name}] ✅ 成功抓取 {url} -> {title}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{threading.current_thread().name}] ❌ 錯誤: {url} -> {e}\")\n",
    "        time.sleep(1)  # 模擬延遲\n",
    "\n",
    "# 建立所有爬蟲執行緒\n",
    "threads = []\n",
    "for url in urls:\n",
    "    t = threading.Thread(target=fetch_url, args=(url,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# 等待所有執行緒完成\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"🎉 所有抓取完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820c4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thread-17 (fetch_url)] 🚀 開始抓取: https://httpbin.org/html\n",
      "[Thread-18 (fetch_url)] 🚀 開始抓取: https://httpbin.org/user-agent\n",
      "[Thread-18 (fetch_url)] ✅ 抓取完成: https://httpbin.org/user-agent → 無標題\n",
      "[Thread-17 (fetch_url)] ✅ 抓取完成: https://httpbin.org/html → 無標題\n",
      "[Thread-19 (fetch_url)] 🚀 開始抓取: https://httpbin.org/headers\n",
      "[Thread-21 (fetch_url)] 🚀 開始抓取: https://httpbin.org/get\n",
      "[Thread-19 (fetch_url)] ✅ 抓取完成: https://httpbin.org/headers → 無標題\n",
      "[Thread-21 (fetch_url)] ✅ 抓取完成: https://httpbin.org/get → 無標題\n",
      "[Thread-20 (fetch_url)] 🚀 開始抓取: https://httpbin.org/ip\n",
      "[Thread-20 (fetch_url)] ✅ 抓取完成: https://httpbin.org/ip → 無標題\n",
      "🎉 所有任務完成\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# 鎖，用於保護 counter 共享資源\n",
    "lock = threading.Lock()\n",
    "\n",
    "# 模擬同時可運行的最大執行緒數量\n",
    "MAX_RUNNING = 2\n",
    "current_running = 0  # 目前正在執行的執行緒數量\n",
    "\n",
    "urls = [\n",
    "    \"https://httpbin.org/html\",\n",
    "    \"https://httpbin.org/user-agent\",\n",
    "    \"https://httpbin.org/headers\",\n",
    "    \"https://httpbin.org/ip\",\n",
    "    \"https://httpbin.org/get\"\n",
    "]\n",
    "\n",
    "def fetch_url(url):\n",
    "    global current_running\n",
    "\n",
    "    # 等待直到有空位\n",
    "    while True:\n",
    "        with lock:\n",
    "            if current_running < MAX_RUNNING:\n",
    "                current_running += 1\n",
    "                break\n",
    "        time.sleep(0.1)  # 稍微等待再嘗試\n",
    "\n",
    "    try:\n",
    "        print(f\"[{threading.current_thread().name}] 🚀 開始抓取: {url}\")\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        title = soup.title.string.strip() if soup.title else \"無標題\"\n",
    "        print(f\"[{threading.current_thread().name}] ✅ 抓取完成: {url} → {title}\")\n",
    "        time.sleep(1)  # 模擬任務花費時間\n",
    "    except Exception as e:\n",
    "        print(f\"[{threading.current_thread().name}] ❌ 抓取錯誤: {url} → {e}\")\n",
    "    finally:\n",
    "        with lock:\n",
    "            current_running -= 1  # 結束時釋放名額\n",
    "\n",
    "# 建立與啟動所有執行緒\n",
    "threads = []\n",
    "for url in urls:\n",
    "    t = threading.Thread(target=fetch_url, args=(url,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# 等待所有執行緒完成\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"🎉 所有任務完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dfe96",
   "metadata": {},
   "source": [
    "### PPT cookie 問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "526eeb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTT 八卦板最新文章標題：\n",
      "[新聞] 自由節研討會 大紀元總裁特邀發表演講\n",
      "[問卦] 擁核自重才是王道對嗎？\n",
      "Re: [問卦] 日本為什麼不跟中國斷交?\n",
      "Re: [新聞] 教師甄選「試教」拿20分惹議 考生痛哭2天：我錯在哪？\n",
      "[新聞] 第6次！彰化「這路口」又有自小客騎上庇\n",
      "Re: [新聞] 曾命中蔡英文會當總統！命理師預言柯文哲\n",
      "Re: [新聞] 黃國昌挑戰司法！播放偵訊朱亞虎「示範帶\n",
      "[問卦] 嘿嘿 雞湯來囉\n",
      "[問卦] 詐騙集團是不是根本不用救?\n",
      "[問卦] 以色列打伊朗是為了主動自衛和人道干涉？\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# PTT 八卦板網址（需要年齡驗證）\n",
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "\n",
    "# 建立 session 用來保持 Cookie\n",
    "session = requests.Session()\n",
    "\n",
    "# 模擬送出「我已滿18歲」的 Cookie\n",
    "session.cookies.set(\"over18\", \"1\")\n",
    "\n",
    "# 送出帶 cookie 的 GET 請求\n",
    "response = session.get(url)\n",
    "\n",
    "# 確認是否成功通過驗證 (PTT會回應正常頁面)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    titles = soup.select(\".title a\")\n",
    "    print(\"PTT 八卦板最新文章標題：\")\n",
    "    for t in titles[:10]:  # 只列出前10篇\n",
    "        print(t.text)\n",
    "else:\n",
    "    print(\"無法存取，可能沒通過年齡驗證或網站異常\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d0504",
   "metadata": {},
   "source": [
    "### PPT cookie 問題 (join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e030f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Thread-5 (fetch_ptt_board)] https://www.ptt.cc/bbs/Baseball/index.html 最新文章標題：\n",
      " - [新聞] MLB／紅襪Campbell遭下放3A！剛獲17.7億\n",
      " - Re: [討論] 昨日富邦跑壘\n",
      " - [分享] 山本由伸第三局11球3K\n",
      " - [新聞] 悍將下半季主題日來了！首度聯名日職關\n",
      " - [新聞] 交易後首度對決 紅襪教頭：德弗斯會想打爆\n",
      "\n",
      "[Thread-4 (fetch_ptt_board)] https://www.ptt.cc/bbs/Tech_Job/index.html 最新文章標題：\n",
      " - [新聞] 華爾街日報：中國工程師赴第三國 租用輝達\n",
      " - [新聞] 亞馬遜執行長賈西開第一槍 示警 AI 導致\n",
      " - [討論] 哪些公司有提供免費的咖啡？\n",
      " - Re: [新聞] AI課太難 大學生爆退選停修\n",
      " - [討論] 南部是不是把主要產業全部押寶上台積了\n",
      "\n",
      "[Thread-3 (fetch_ptt_board)] https://www.ptt.cc/bbs/Gossiping/index.html 最新文章標題：\n",
      " - [問卦] 30、40歲沒存款的是花去哪？？\n",
      " - Re: [問卦] 到底為什麼不對中共開火？？？\n",
      " - Re: [新聞] 死囚鄭武松判死定讞20年 聲請再審並停止\n",
      " - [問卦] 月薪48K管理職在高雄PR大概多少？\n",
      " - [問卦] 移工妹子都用同牌洗衣精洗髮精沐浴乳?\n",
      "\n",
      "所有板的文章標題抓取完成！\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 要爬的 PTT 看板或頁面列表（示範幾個板）\n",
    "urls = [\n",
    "    \"https://www.ptt.cc/bbs/Gossiping/index.html\",\n",
    "    \"https://www.ptt.cc/bbs/Tech_Job/index.html\",\n",
    "    \"https://www.ptt.cc/bbs/Baseball/index.html\"\n",
    "]\n",
    "\n",
    "def fetch_ptt_board(url):\n",
    "    # 每個執行緒都建立獨立的 session\n",
    "    session = requests.Session()\n",
    "    # 模擬已通過 18 歲驗證 Cookie\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "\n",
    "    try:\n",
    "        res = session.get(url, timeout=10)\n",
    "        if res.status_code == 200:\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            titles = soup.select(\".title a\")\n",
    "            print(f\"\\n[{threading.current_thread().name}] {url} 最新文章標題：\")\n",
    "            for t in titles[:5]:  # 顯示前5篇標題\n",
    "                print(\" -\", t.text)\n",
    "        else:\n",
    "            print(f\"[{threading.current_thread().name}] 無法存取 {url}，HTTP狀態碼：{res.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{threading.current_thread().name}] 發生錯誤: {e}\")\n",
    "\n",
    "threads = []\n",
    "\n",
    "# 建立並啟動執行緒\n",
    "for url in urls:\n",
    "    t = threading.Thread(target=fetch_ptt_board, args=(url,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# 等待所有執行緒結束\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"\\n所有板的文章標題抓取完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83497e7d",
   "metadata": {},
   "source": [
    "### PPT cookie 問題 (單執行緒)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9d09ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 多執行緒爬取 ===\n",
      "[Thread-31 (fetch_ptt_board)] https://www.ptt.cc/bbs/Gossiping/index.html 最新文章標題：\n",
      " - [問卦] 講到BMI為什麼就會有人生氣？\n",
      " - [問卦] 為什麼銀樓都喜歡收現金啊？\n",
      " - [問卦] windows 介面怎麼這麼醜?\n",
      "[Thread-32 (fetch_ptt_board)] https://www.ptt.cc/bbs/Tech_Job/index.html 最新文章標題：\n",
      " - [新聞] 華爾街日報：中國工程師赴第三國 租用輝達\n",
      " - [新聞] 亞馬遜執行長賈西開第一槍 示警 AI 導致\n",
      " - [討論] 哪些公司有提供免費的咖啡？\n",
      "[Thread-33 (fetch_ptt_board)] https://www.ptt.cc/bbs/Baseball/index.html 最新文章標題：\n",
      " - [情報] 道奇隊拒絕ICE探員進入球場\n",
      " - [分享] Tony Gonsolin 移往60天傷兵名單\n",
      " - [分享] 今日Logan Webb 7.0IP/1R/9K\n",
      "\n",
      "多執行緒花費時間: 0.65 秒\n",
      "\n",
      "=== 單執行緒爬取 ===\n",
      "[MainThread] https://www.ptt.cc/bbs/Gossiping/index.html 最新文章標題：\n",
      " - [問卦] 講到BMI為什麼就會有人生氣？\n",
      " - [問卦] 為什麼銀樓都喜歡收現金啊？\n",
      " - [問卦] windows 介面怎麼這麼醜?\n",
      "[MainThread] https://www.ptt.cc/bbs/Tech_Job/index.html 最新文章標題：\n",
      " - [新聞] 華爾街日報：中國工程師赴第三國 租用輝達\n",
      " - [新聞] 亞馬遜執行長賈西開第一槍 示警 AI 導致\n",
      " - [討論] 哪些公司有提供免費的咖啡？\n",
      "[MainThread] https://www.ptt.cc/bbs/Baseball/index.html 最新文章標題：\n",
      " - [情報] 道奇隊拒絕ICE探員進入球場\n",
      " - [分享] Tony Gonsolin 移往60天傷兵名單\n",
      " - [分享] 今日Logan Webb 7.0IP/1R/9K\n",
      "\n",
      "單執行緒花費時間: 1.87 秒\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "urls = [\n",
    "    \"https://www.ptt.cc/bbs/Gossiping/index.html\",\n",
    "    \"https://www.ptt.cc/bbs/Tech_Job/index.html\",\n",
    "    \"https://www.ptt.cc/bbs/Baseball/index.html\"\n",
    "]\n",
    "\n",
    "def fetch_ptt_board(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        titles = soup.select(\".title a\")\n",
    "        print(f\"[{threading.current_thread().name}] {url} 最新文章標題：\")\n",
    "        for t in titles[:3]:\n",
    "            print(\" -\", t.text)\n",
    "    else:\n",
    "        print(f\"[{threading.current_thread().name}] 無法存取 {url}\")\n",
    "\n",
    "def multi_thread_crawl():\n",
    "    threads = []\n",
    "    start = time.time()\n",
    "\n",
    "    for url in urls:\n",
    "        t = threading.Thread(target=fetch_ptt_board, args=(url,))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\n多執行緒花費時間: {end - start:.2f} 秒\")\n",
    "\n",
    "def single_thread_crawl():\n",
    "    start = time.time()\n",
    "    for url in urls:\n",
    "        fetch_ptt_board(url)\n",
    "    end = time.time()\n",
    "    print(f\"\\n單執行緒花費時間: {end - start:.2f} 秒\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== 多執行緒爬取 ===\")\n",
    "    multi_thread_crawl()\n",
    "    print(\"\\n=== 單執行緒爬取 ===\")\n",
    "    single_thread_crawl()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0265ad4",
   "metadata": {},
   "source": [
    "### PPT cookie 問題 + 內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da156775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[文章標題] [問卦] 講到BMI為什麼就會有人生氣？\n",
      "\n",
      "[文章內容] https://www.ptt.cc/bbs/Gossiping/M.1750389849.A.B44.html：\n",
      "每次講到BMI都會有人氣得跳腳\n",
      "\n",
      "說BMI沒用 泡芙人看不出來\n",
      "\n",
      "可是到現在醫學仍然是用BMI來判斷肥胖 過重標準\n",
      "\n",
      "而且台灣巨巨也沒這麼多\n",
      "\n",
      "\n",
      "\n",
      "用\"體脂\"來當標準的話\n",
      "\n",
      "19歲以上成人中，過重以上人口佔全體人口之百分比\n",
      "*男性體脂肪≧25%，女性體脂肪≧30%\n",
      "https://i.imgur.com/SCs7smX.png\n",
      "依據上述體脂率標準\n",
      "男性年輕的六成過重\n",
      "中老年八成\n",
      "\n",
      "女性年輕的八成過重\n",
      "中老年九成\n",
      "\n",
      "好像比用BMI更慘\n",
      "\n",
      "\n",
      "BMI是一塊遮羞布嗎\n",
      "\n",
      "有沒有八卦？\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 220.128.198.204 (臺灣)\n",
      "※ 文章網址:\n",
      "...\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "[文章標題] [問卦] 為什麼銀樓都喜歡收現金啊？\n",
      "\n",
      "[文章內容] https://www.ptt.cc/bbs/Gossiping/M.1750389850.A.A12.html：\n",
      "去銀樓購買金飾的時候，想用信用卡結帳，\n",
      "卻被老闆告知：\n",
      "如果刷卡要被收3%，2萬要收600，\n",
      "現金付款對我比較有利\n",
      "\n",
      "為了省下小錢也是去提領了，\n",
      "刷卡的回扣也比不上3%就是了\n",
      "\n",
      "現金能讓他們逃漏多少稅啊？\n",
      "5%的營業稅，還是多少營所稅？\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 42.76.160.5 (臺灣)\n",
      "※ 文章網址:\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1750389850.A.A12.html...\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "[文章標題] [問卦] windows 介面怎麼這麼醜?\n",
      "\n",
      "[文章內容] https://www.ptt.cc/bbs/Gossiping/M.1750389864.A.EC1.html：\n",
      "mac os這種的不說 畢竟果子美學還是在線的\n",
      "\n",
      "但是隨便一個linux發行版 UBUNTU FEDORA\n",
      "\n",
      "那個UI 字體都屌打WINDOWS\n",
      "\n",
      "軟軟內部沒有人才嗎?設計一個好看的桌面環境很難?\n",
      "\n",
      "為什麼WINDOWS看起來這麼醜?有無八卦?\n",
      "\n",
      "--\n",
      "如何嘴砲\n",
      "反駁對方的重點──◢◣\n",
      "█\n",
      "確實指出人家論點的錯誤性\n",
      "ψQSWEET\n",
      "│\n",
      "＞  ◎\n",
      "駁斥\n",
      "──────\n",
      "◢\n",
      "◣\n",
      "█\n",
      "用引言指出對方錯誤或矛盾的地方\n",
      "(\n",
      "█\n",
      "優質論文）\n",
      "在嘴砲王\n",
      "相反的觀點\n",
      "──\n",
      "◢████◣\n",
      "█\n",
      "列出相反的論點並以事實當證據\n",
      "(\n",
      "█\n",
      "辯論社）\n",
      "應該出現\n",
      "⊙\n",
      "矛盾\n",
      "────\n",
      "◢██████◣\n",
      "█\n",
      "列出相反的論點但不加以...\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "[文章標題] Re: [問卦] 韓國歐巴 1992年才跟中華民國斷交？\n",
      "\n",
      "[文章內容] https://www.ptt.cc/bbs/Gossiping/M.1750389867.A.7B7.html：\n",
      "※ 引述《seabox (呂雅筑)》之銘言：\n",
      ": 美帝跟日寇\n",
      ": 還有美國的歐洲走狗們\n",
      ": 都早早跟我們中華民國（中國）斷交\n",
      ": 韓國歐巴\n",
      ": 直到1992年都還承認中華民國就是中國\n",
      ": 為什麼韓國歐巴那麼久才斷交呢\n",
      "如果你有跟亞洲各國人交往\n",
      "\n",
      "你就會發現\n",
      "\n",
      "雖然一直喊台日友好 但是那更多的只是台灣人的一廂情願\n",
      "\n",
      "\n",
      "其實韓國人個性跟台灣人是比較像的\n",
      "韓國人是真的把你當兄弟那種\n",
      "\n",
      "\n",
      "-----\n",
      "Sent from JPTT on my Xiaomi 2201117SY.\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 49.217.62.212 (臺灣)\n",
      "※ 文章網址:\n",
      "https...\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "[文章標題] [問卦] 中華民國臺灣那麼棒怎沒朋友？\n",
      "\n",
      "[文章內容] https://www.ptt.cc/bbs/Gossiping/M.1750389868.A.D73.html：\n",
      "如題啦\n",
      "中華民國臺灣\n",
      "有台積電\n",
      "有超好吃的小吃\n",
      "有超善良的人\n",
      "超反共\n",
      "怎麼沒有國家\n",
      "要跟我們做朋友\n",
      "只會講友好場面話\n",
      "有沒有卦\n",
      "QQ\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 49.218.209.251 (臺灣)\n",
      "※ 文章網址:\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1750389868.A.D73.html...\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "總花費時間: 3.67 秒\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://www.ptt.cc\"\n",
    "BOARD_URL = BASE_URL + \"/bbs/Gossiping/index.html\"\n",
    "\n",
    "# 進入文章爬內文的函式，這裡同步執行，會等待此函式完成後才繼續\n",
    "def fetch_article_content(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        main_content = soup.find(id=\"main-content\")\n",
    "        # 移除文章內的留言區等不必要內容\n",
    "        for tag in main_content.find_all(['div', 'span', 'a'], class_=['article-metaline', 'article-metaline-right', 'push']):\n",
    "            tag.decompose()\n",
    "        text = main_content.get_text(strip=True, separator=\"\\n\")\n",
    "        print(f\"\\n[文章內容] {url}：\\n{text[:300]}...\")  # 只印前300字\n",
    "    else:\n",
    "        print(f\"無法取得文章內容：{url}\")\n",
    "\n",
    "def fetch_board_articles():\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(BOARD_URL, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        print(\"無法取得看板頁面\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = soup.select(\".title a\")\n",
    "\n",
    "    for a in articles[:5]:  # 先抓前5篇示範\n",
    "        title = a.text\n",
    "        href = a['href']\n",
    "        article_url = BASE_URL + href\n",
    "        print(f\"\\n[文章標題] {title}\")\n",
    "        \n",
    "        # 建立執行緒去爬文章內文\n",
    "        t = threading.Thread(target=fetch_article_content, args=(article_url,))\n",
    "        t.start()\n",
    "        # 用 join 等待此文章內文爬完，再繼續下一篇\n",
    "        t.join()\n",
    "        print(\"=== 文章抓取完成 ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    fetch_board_articles()\n",
    "    print(f\"\\n總花費時間: {time.time() - start_time:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302ed9f",
   "metadata": {},
   "source": [
    "### PPT + 看板 + 內容 (lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70d89475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[文章標題] [問卦] 講到BMI為什麼就會有人生氣？\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "[文章標題] [問卦] 為什麼銀樓都喜歡收現金啊？\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "[文章標題] [問卦] windows 介面怎麼這麼醜?\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "[文章標題] Re: [問卦] 韓國歐巴 1992年才跟中華民國斷交？\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "[文章標題] [問卦] 中華民國臺灣那麼棒怎沒朋友？\n",
      "=== 文章抓取完成 ===\n",
      "\n",
      "總花費時間: 3.64 秒\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "BASE_URL = \"https://www.ptt.cc\"\n",
    "BOARD_URL = BASE_URL + \"/bbs/Gossiping/index.html\"\n",
    "\n",
    "results = []       # 全部文章資料會存這裡\n",
    "results_lock = threading.Lock()  # 保護 results 資料結構的鎖\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        main_content = soup.find(id=\"main-content\")\n",
    "        for tag in main_content.find_all(['div', 'span', 'a'], class_=['article-metaline', 'article-metaline-right', 'push']):\n",
    "            tag.decompose()\n",
    "        text = main_content.get_text(strip=True, separator=\"\\n\")\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def fetch_board_articles():\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(BOARD_URL, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        print(\"無法取得看板頁面\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = soup.select(\".title a\")\n",
    "\n",
    "    for a in articles[:5]:  # 先抓前5篇示範\n",
    "        title = a.text\n",
    "        href = a['href']\n",
    "        article_url = BASE_URL + href\n",
    "        print(f\"\\n[文章標題] {title}\")\n",
    "\n",
    "        # 用執行緒抓文章內文並取得結果\n",
    "        content = None\n",
    "\n",
    "        def thread_job():\n",
    "            nonlocal content\n",
    "            content = fetch_article_content(article_url)\n",
    "\n",
    "        t = threading.Thread(target=thread_job)\n",
    "        t.start()\n",
    "        t.join()\n",
    "\n",
    "        print(\"=== 文章抓取完成 ===\")\n",
    "\n",
    "        # 用鎖保護 results 的修改\n",
    "        with results_lock:\n",
    "            results.append({\n",
    "                \"title\": title,\n",
    "                \"url\": article_url,\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    fetch_board_articles()\n",
    "    print(f\"\\n總花費時間: {time.time() - start_time:.2f} 秒\")\n",
    "\n",
    "    # 存成 JSON 檔案\n",
    "    # with open(\"ptt_gossiping_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    # print(\"資料已存成 ptt_gossiping_articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72b42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '[問卦] 講到BMI為什麼就會有人生氣？',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389849.A.B44.html',\n",
       "  'content': '每次講到BMI都會有人氣得跳腳\\n\\n說BMI沒用 泡芙人看不出來\\n\\n可是到現在醫學仍然是用BMI來判斷肥胖 過重標準\\n\\n而且台灣巨巨也沒這麼多\\n\\n\\n\\n用\"體脂\"來當標準的話\\n\\n19歲以上成人中，過重以上人口佔全體人口之百分比\\n*男性體脂肪≧25%，女性體脂肪≧30%\\nhttps://i.imgur.com/SCs7smX.png\\n依據上述體脂率標準\\n男性年輕的六成過重\\n中老年八成\\n\\n女性年輕的八成過重\\n中老年九成\\n\\n好像比用BMI更慘\\n\\n\\nBMI是一塊遮羞布嗎\\n\\n有沒有八卦？\\n\\n--\\n※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 220.128.198.204 (臺灣)\\n※ 文章網址:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389849.A.B44.html'},\n",
       " {'title': '[問卦] 為什麼銀樓都喜歡收現金啊？',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389850.A.A12.html',\n",
       "  'content': '去銀樓購買金飾的時候，想用信用卡結帳，\\n卻被老闆告知：\\n如果刷卡要被收3%，2萬要收600，\\n現金付款對我比較有利\\n\\n為了省下小錢也是去提領了，\\n刷卡的回扣也比不上3%就是了\\n\\n現金能讓他們逃漏多少稅啊？\\n5%的營業稅，還是多少營所稅？\\n\\n--\\n※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 42.76.160.5 (臺灣)\\n※ 文章網址:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389850.A.A12.html'},\n",
       " {'title': '[問卦] windows 介面怎麼這麼醜?',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389864.A.EC1.html',\n",
       "  'content': 'mac os這種的不說 畢竟果子美學還是在線的\\n\\n但是隨便一個linux發行版 UBUNTU FEDORA\\n\\n那個UI 字體都屌打WINDOWS\\n\\n軟軟內部沒有人才嗎?設計一個好看的桌面環境很難?\\n\\n為什麼WINDOWS看起來這麼醜?有無八卦?\\n\\n--\\n如何嘴砲\\n反駁對方的重點──◢◣\\n█\\n確實指出人家論點的錯誤性\\nψQSWEET\\n│\\n＞  ◎\\n駁斥\\n──────\\n◢\\n◣\\n█\\n用引言指出對方錯誤或矛盾的地方\\n(\\n█\\n優質論文）\\n在嘴砲王\\n相反的觀點\\n──\\n◢████◣\\n█\\n列出相反的論點並以事實當證據\\n(\\n█\\n辯論社）\\n應該出現\\n⊙\\n矛盾\\n────\\n◢██████◣\\n█\\n列出相反的論點但不加以證實\\n(\\n█\\n█\\n論壇)\\n的元素\\n攻擊態度\\n─\\n◢████████◣\\n█\\n質疑對方的態度和口氣\\n(\\n█\\n█\\n匿名版)\\n人身攻擊\\n↘\\n偏見\\n↗\\n▄\\n▄▄▄▄▄▄▄▄▄▄▄\\n█\\n攻擊身份和能耐\\n█\\n幹你娘\\n(\\n█\\n小朋友)\\n--\\n※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 36.228.227.41 (臺灣)\\n※ 文章網址:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389864.A.EC1.html'},\n",
       " {'title': 'Re: [問卦] 韓國歐巴 1992年才跟中華民國斷交？',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389867.A.7B7.html',\n",
       "  'content': '※ 引述《seabox (呂雅筑)》之銘言：\\n: 美帝跟日寇\\n: 還有美國的歐洲走狗們\\n: 都早早跟我們中華民國（中國）斷交\\n: 韓國歐巴\\n: 直到1992年都還承認中華民國就是中國\\n: 為什麼韓國歐巴那麼久才斷交呢\\n如果你有跟亞洲各國人交往\\n\\n你就會發現\\n\\n雖然一直喊台日友好 但是那更多的只是台灣人的一廂情願\\n\\n\\n其實韓國人個性跟台灣人是比較像的\\n韓國人是真的把你當兄弟那種\\n\\n\\n-----\\nSent from JPTT on my Xiaomi 2201117SY.\\n\\n--\\n※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 49.217.62.212 (臺灣)\\n※ 文章網址:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389867.A.7B7.html'},\n",
       " {'title': '[問卦] 中華民國臺灣那麼棒怎沒朋友？',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389868.A.D73.html',\n",
       "  'content': '如題啦\\n中華民國臺灣\\n有台積電\\n有超好吃的小吃\\n有超善良的人\\n超反共\\n怎麼沒有國家\\n要跟我們做朋友\\n只會講友好場面話\\n有沒有卦\\nQQ\\n\\n--\\n※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 49.218.209.251 (臺灣)\\n※ 文章網址:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389868.A.D73.html'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae0d22",
   "metadata": {},
   "source": [
    "### PPT + 多看板 + 內容 (lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32762704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 開始抓取看板：Gossiping ====\n",
      "[Gossiping] 文章標題: [問卦] 30、40歲沒存款的是花去哪？？\n",
      "[Gossiping] 文章內容抓取完成\n",
      "[Gossiping] 文章標題: Re: [問卦] 到底為什麼不對中共開火？？？\n",
      "[Gossiping] 文章內容抓取完成\n",
      "[Gossiping] 文章標題: Re: [新聞] 死囚鄭武松判死定讞20年 聲請再審並停止\n",
      "[Gossiping] 文章內容抓取完成\n",
      "[Gossiping] 文章標題: [問卦] 月薪48K管理職在高雄PR大概多少？\n",
      "[Gossiping] 文章內容抓取完成\n",
      "[Gossiping] 文章標題: [問卦] 移工妹子都用同牌洗衣精洗髮精沐浴乳?\n",
      "[Gossiping] 文章內容抓取完成\n",
      "==== 完成看板：Gossiping ====\n",
      "\n",
      "==== 開始抓取看板：Baseball ====\n",
      "[Baseball] 文章標題: [新聞] MLB／紅襪Campbell遭下放3A！剛獲17.7億\n",
      "[Baseball] 文章內容抓取完成\n",
      "[Baseball] 文章標題: Re: [討論] 昨日富邦跑壘\n",
      "[Baseball] 文章內容抓取完成\n",
      "[Baseball] 文章標題: [分享] 山本由伸第三局11球3K\n",
      "[Baseball] 文章內容抓取完成\n",
      "[Baseball] 文章標題: [新聞] 悍將下半季主題日來了！首度聯名日職關\n",
      "[Baseball] 文章內容抓取完成\n",
      "[Baseball] 文章標題: [新聞] 交易後首度對決 紅襪教頭：德弗斯會想打爆\n",
      "[Baseball] 文章內容抓取完成\n",
      "==== 完成看板：Baseball ====\n",
      "\n",
      "==== 開始抓取看板：Beauty ====\n",
      "[Beauty] 文章標題: [正妹] Cospaly 2044 韓國 女巫 妮姬\n",
      "[Beauty] 文章內容抓取完成\n",
      "[Beauty] 文章標題: [正妹] 三橋くん\n",
      "[Beauty] 文章內容抓取完成\n",
      "[Beauty] 文章標題: [正妹] 有幾顆保齡球\n",
      "[Beauty] 文章內容抓取完成\n",
      "[Beauty] 文章標題: [正妹] 林芷芸\n",
      "[Beauty] 文章內容抓取完成\n",
      "[Beauty] 文章標題: [正妹] 正又身材好\n",
      "[Beauty] 文章內容抓取完成\n",
      "==== 完成看板：Beauty ====\n",
      "\n",
      "總花費時間: 11.10 秒\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "BASE_URL = \"https://www.ptt.cc\"\n",
    "BOARDS = {\n",
    "    \"Gossiping\": \"/bbs/Gossiping/index.html\",\n",
    "    \"Baseball\": \"/bbs/Baseball/index.html\",\n",
    "    \"Beauty\": \"/bbs/Beauty/index.html\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "results_lock = threading.Lock()\n",
    "board_lock = threading.Lock()  # 用來保護看板級別的執行流程\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        main_content = soup.find(id=\"main-content\")\n",
    "        for tag in main_content.find_all(['div', 'span', 'a'], class_=['article-metaline', 'article-metaline-right', 'push']):\n",
    "            tag.decompose()\n",
    "        text = main_content.get_text(strip=True, separator=\"\\n\")\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def fetch_board_articles(board_name, board_path):\n",
    "    print(f\"\\n==== 開始抓取看板：{board_name} ====\")\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    url = BASE_URL + board_path\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"無法取得看板頁面 {board_name}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = soup.select(\".title a\")\n",
    "\n",
    "    for a in articles[:5]:  # 先抓5篇示範\n",
    "        title = a.text\n",
    "        href = a['href']\n",
    "        article_url = BASE_URL + href\n",
    "        print(f\"[{board_name}] 文章標題: {title}\")\n",
    "\n",
    "        content = None\n",
    "        def thread_job():\n",
    "            nonlocal content\n",
    "            content = fetch_article_content(article_url)\n",
    "\n",
    "        # 一篇文章用一個執行緒抓取文章內容\n",
    "        t = threading.Thread(target=thread_job)\n",
    "        t.start()\n",
    "        t.join()  # 等該篇文章抓完才繼續下一篇\n",
    "\n",
    "        print(f\"[{board_name}] 文章內容抓取完成\")\n",
    "\n",
    "        with results_lock:\n",
    "            results.append({\n",
    "                \"board\": board_name,\n",
    "                \"title\": title,\n",
    "                \"url\": article_url,\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "    print(f\"==== 完成看板：{board_name} ====\")\n",
    "\n",
    "def main():\n",
    "    for board_name, board_path in BOARDS.items():\n",
    "        # 用board_lock確保一次只能一個看板在抓\n",
    "        with board_lock:\n",
    "            fetch_board_articles(board_name, board_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    print(f\"\\n總花費時間: {time.time() - start_time:.2f} 秒\")\n",
    "\n",
    "    # with open(\"ptt_multiple_boards.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # print(\"資料已存成 ptt_multiple_boards.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edace2fa",
   "metadata": {},
   "source": [
    "### PPT + 看板 + 內容 (單執行緒)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab89faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 開始抓取看板：Gossiping ====\n",
      "[Gossiping] 文章標題: [問卦] 講到BMI為什麼就會有人生氣？\n",
      "[Gossiping] 文章內容抓取完成\n",
      "[Gossiping] 文章標題: [問卦] 為什麼銀樓都喜歡收現金啊？\n",
      "[Gossiping] 文章內容抓取完成\n",
      "[Gossiping] 文章標題: [問卦] windows 介面怎麼這麼醜?\n",
      "[Gossiping] 文章內容抓取完成\n",
      "[Gossiping] 文章標題: Re: [問卦] 韓國歐巴 1992年才跟中華民國斷交？\n",
      "[Gossiping] 文章內容抓取完成\n",
      "[Gossiping] 文章標題: [問卦] 中華民國臺灣那麼棒怎沒朋友？\n",
      "[Gossiping] 文章內容抓取完成\n",
      "==== 完成看板：Gossiping ====\n",
      "\n",
      "==== 開始抓取看板：Baseball ====\n",
      "[Baseball] 文章標題: [新聞] MLB／紅襪Campbell遭下放3A！剛獲17.7億\n",
      "[Baseball] 文章內容抓取完成\n",
      "[Baseball] 文章標題: [公告] 板主徵選開始\n",
      "[Baseball] 文章內容抓取完成\n",
      "[Baseball] 文章標題: [公告] 板規 v7.0\n",
      "[Baseball] 文章內容抓取完成\n",
      "[Baseball] 文章標題: [整理] 2025 棒球賽事 轉播時間表\n",
      "[Baseball] 文章內容抓取完成\n",
      "==== 完成看板：Baseball ====\n",
      "\n",
      "==== 開始抓取看板：Beauty ====\n",
      "[Beauty] 文章標題: [正妹] Cospaly 2044 韓國 女巫 妮姬\n",
      "[Beauty] 文章內容抓取完成\n",
      "[Beauty] 文章標題: [正妹] 三橋くん\n",
      "[Beauty] 文章內容抓取完成\n",
      "[Beauty] 文章標題: [正妹] 有幾顆保齡球\n",
      "[Beauty] 文章內容抓取完成\n",
      "[Beauty] 文章標題: [正妹] 林芷芸\n",
      "[Beauty] 文章內容抓取完成\n",
      "[Beauty] 文章標題: [正妹] 正又身材好\n",
      "[Beauty] 文章內容抓取完成\n",
      "==== 完成看板：Beauty ====\n",
      "\n",
      "總花費時間: 10.58 秒\n",
      "資料已存成 ptt_single_thread.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "BASE_URL = \"https://www.ptt.cc\"\n",
    "BOARDS = {\n",
    "    \"Gossiping\": \"/bbs/Gossiping/index.html\",\n",
    "    \"Baseball\": \"/bbs/Baseball/index.html\",\n",
    "    \"Beauty\": \"/bbs/Beauty/index.html\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        main_content = soup.find(id=\"main-content\")\n",
    "        for tag in main_content.find_all(['div', 'span', 'a'], class_=['article-metaline', 'article-metaline-right', 'push']):\n",
    "            tag.decompose()\n",
    "        text = main_content.get_text(strip=True, separator=\"\\n\")\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def fetch_board_articles(board_name, board_path):\n",
    "    print(f\"\\n==== 開始抓取看板：{board_name} ====\")\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    url = BASE_URL + board_path\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"無法取得看板頁面 {board_name}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = soup.select(\".title a\")\n",
    "\n",
    "    for a in articles[:5]:  # 先抓5篇示範\n",
    "        title = a.text\n",
    "        href = a['href']\n",
    "        article_url = BASE_URL + href\n",
    "        print(f\"[{board_name}] 文章標題: {title}\")\n",
    "\n",
    "        # 單執行緒直接呼叫\n",
    "        content = fetch_article_content(article_url)\n",
    "        print(f\"[{board_name}] 文章內容抓取完成\")\n",
    "\n",
    "        results.append({\n",
    "            \"board\": board_name,\n",
    "            \"title\": title,\n",
    "            \"url\": article_url,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    print(f\"==== 完成看板：{board_name} ====\")\n",
    "\n",
    "def main():\n",
    "    for board_name, board_path in BOARDS.items():\n",
    "        fetch_board_articles(board_name, board_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    print(f\"\\n總花費時間: {time.time() - start_time:.2f} 秒\")\n",
    "\n",
    "    # with open(\"ptt_single_thread.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"資料已存成 ptt_single_thread.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8abaf9",
   "metadata": {},
   "source": [
    "## 資料庫\n",
    "\n",
    "- Python 的 threading 模組特別適用於 I/O-bound 工作，像是：\n",
    "    - 網路爬蟲（等待網頁回應）\n",
    "    - 檔案讀寫\n",
    "    - 資料庫操作\n",
    "    - 等待硬碟、網路、API 等外部設備\n",
    "    - 這些工作本身不會吃太多 CPU，但會讓主程式「卡住等待」，這時用多執行緒可以「一邊等，一邊做別的事情」，大幅減少總時間。\n",
    "- 避免記憶體 Crash？不是主要目的，但有關係\n",
    "    - 多執行緒本身並不會特別節省記憶體，反而會因為多個執行緒同時存在：\n",
    "        - 佔用更多記憶體（每個 thread 有自己的堆疊）\n",
    "        - 若沒管控好，可能會導致資源爭奪、死鎖、記憶體問題\n",
    "> 🧯但你提到「避免記憶體 crash」這件事，是在有大量任務或不當同步時，有些人會**使用多執行緒 + 限流（像是 Semaphore）**來避免開太多資源導致崩潰，這是控制風險，不是本質目的。\n",
    "\n",
    "| 目的                | 是否主要目的 | 補充說明                        |\n",
    "| ----------------- | ------ | --------------------------- |\n",
    "| ✅ 節省總執行時間         | ✅ 是    | 特別對 I/O-bound 任務            |\n",
    "| ⚠️ 資源分配避免 crash   | ❌ 否    | 不是本意，但多執行緒可搭配控制機制來避免        |\n",
    "| ❌ 加速 CPU-heavy 工作 | ❌ 否    | 應該用多處理程序（`multiprocessing`） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c39fda",
   "metadata": {},
   "source": [
    "### 同時連三個表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f207de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "所有查詢完成，耗時：3.77 秒\n",
      "sponsor_view: 2261 rows\n",
      "ana_table: 7974 rows\n",
      "orders: 8225 rows\n",
      "order_lines: 8094 rows\n",
      "league_map: 10342 rows\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "def query_table(table_name):\n",
    "    try:\n",
    "        conn = pymysql.connect(\n",
    "            host=\"HOST屏蔽\",           # 加上你的 DB 主機 IP 或名稱\n",
    "            user=\"eds\",\n",
    "            password=\"!2018Eds\",       # 加上密碼\n",
    "            database=\"sports_unify_db\",\n",
    "            charset=\"utf8mb4\",\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "        results = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "        return table_name, df\n",
    "    except Exception as e:\n",
    "        print(f\"[DataReader] Failed to read table `{table_name}`: {e}\")\n",
    "        return table_name, None\n",
    "    finally:\n",
    "        if \"conn\" in locals():\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def query_multiple_tables_parallel(table_names):\n",
    "    all_results = {}\n",
    "\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:  # 最多同時查詢 5 張表\n",
    "        futures = [executor.submit(query_table, table) for table in table_names]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            table_name, df = future.result()\n",
    "            all_results[table_name] = df\n",
    "\n",
    "    print(f\"\\n所有查詢完成，耗時：{time.time() - start_time:.2f} 秒\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables = [\"ana_table\", \"sponsor_view\", \"order_lines\", \"orders\", \"league_map\"]#, \"match_map\", \"team_map\"]  # 自行替換成你有的表\n",
    "    # tables = [\"ana_table\", \"sponsor_view\", \"order_lines\", \"match_map\", \"team_map\"]\n",
    "    results = query_multiple_tables_parallel(tables)\n",
    "\n",
    "    # 範例：顯示每張表的筆數\n",
    "    for table, df in results.items():\n",
    "        if df is not None:\n",
    "            print(f\"{table}: {len(df)} rows\")\n",
    "        else:\n",
    "            print(f\"{table}: ❌ 查詢失敗\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeed604",
   "metadata": {},
   "source": [
    "### 同時連三個表 (設定匯入次數)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771290f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 ana_table: 正在查詢第 0 筆開始的資料...\n",
      "🔍 sponsor_view: 正在查詢第 0 筆開始的資料...\n",
      "🔍 order_lines: 正在查詢第 0 筆開始的資料...\n",
      "🔍 orders: 正在查詢第 0 筆開始的資料...\n",
      "🔍 league_map: 正在查詢第 0 筆開始的資料...\n",
      "🔍 sponsor_view: 正在查詢第 12000 筆開始的資料...\n",
      "🔍 ana_table: 正在查詢第 12000 筆開始的資料...\n",
      "🔍 orders: 正在查詢第 12000 筆開始的資料...\n",
      "🔍 order_lines: 正在查詢第 12000 筆開始的資料...\n",
      "🔍 league_map: 正在查詢第 12000 筆開始的資料...\n",
      "\n",
      "所有查詢完成，總耗時：4.08 秒\n",
      "sponsor_view: 2261 rows\n",
      "ana_table: 7974 rows\n",
      "orders: 8225 rows\n",
      "order_lines: 8094 rows\n",
      "league_map: 10342 rows\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# 建立連線池引擎\n",
    "engine = create_engine(\"mysql+pymysql://密碼屏蔽@HOST屏蔽/sports_unify_db\", pool_size=100)# 最大是500\n",
    "\n",
    "def paginated_query(table_name, batch_size=12000):\n",
    "    offset = 0\n",
    "    df_list = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            sql = f\"SELECT * FROM {table_name} LIMIT {batch_size} OFFSET {offset}\"\n",
    "            print(f\"🔍 {table_name}: 正在查詢第 {offset} 筆開始的資料...\")\n",
    "            try:\n",
    "                df = pd.read_sql(sql, engine)\n",
    "            except Exception as e:\n",
    "                print(f\"[查詢錯誤] {table_name} offset {offset}：{e}\")\n",
    "                break\n",
    "\n",
    "            if df.empty:\n",
    "                break\n",
    "\n",
    "            df_list.append(df)\n",
    "            offset += batch_size\n",
    "\n",
    "        if df_list:\n",
    "            final_df = pd.concat(df_list, ignore_index=True)\n",
    "            return table_name, final_df\n",
    "        else:\n",
    "            return table_name, pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[致命錯誤] {table_name} 查詢失敗：{e}\")\n",
    "        return table_name, None\n",
    "    \n",
    "def query_multiple_tables_parallel(table_names, max_workers=4):\n",
    "    all_results = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(paginated_query, table) for table in table_names]\n",
    "        for future in as_completed(futures):\n",
    "            table_name, df = future.result()\n",
    "            all_results[table_name] = df\n",
    "\n",
    "\n",
    "    print(f\"\\n所有查詢完成，總耗時：{time.time() - start_time:.2f} 秒\")\n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables = [\"ana_table\", \"sponsor_view\", \"order_lines\", \"orders\", \"league_map\"]\n",
    "    max_thread_workers = 6  # 可在這裡自由調整 worker 數\n",
    "    results = query_multiple_tables_parallel(tables, max_workers=max_thread_workers)\n",
    "\n",
    "    for table, df in results.items():\n",
    "        if df is not None:\n",
    "            print(f\"{table}: {len(df)} rows\")\n",
    "        else:\n",
    "            print(f\"{table}: 查詢失敗\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 ana_table: 正在查詢第 0 筆開始的資料...\n",
      "🔍 sponsor_view: 正在查詢第 0 筆開始的資料...\n",
      "🔍 order_lines: 正在查詢第 0 筆開始的資料...\n",
      "🔍 orders: 正在查詢第 0 筆開始的資料...\n",
      "🔍 sponsor_view: 正在查詢第 12000 筆開始的資料...\n",
      "🔍 ana_table: 正在查詢第 12000 筆開始的資料...\n",
      "🔍 orders: 正在查詢第 12000 筆開始的資料...\n",
      "🔍 order_lines: 正在查詢第 12000 筆開始的資料...\n",
      "\n",
      "所有查詢完成，總耗時：3.48 秒\n",
      "sponsor_view: 2261 rows\n",
      "ana_table: 7974 rows\n",
      "orders: 8225 rows\n",
      "order_lines: 8094 rows\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# 建立連線池引擎\n",
    "engine = create_engine(\"mysql+pymysql://密碼屏蔽@HOST屏蔽/sports_unify_db\", pool_size=100)  # 最大是500\n",
    "\n",
    "def paginated_query(table_name, batch_size=12000):\n",
    "    offset = 0\n",
    "    dfs = []  # 用 list 收集，避免反覆append造成效能問題\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            sql = f\"SELECT * FROM {table_name} LIMIT {batch_size} OFFSET {offset}\"\n",
    "            print(f\"🔍 {table_name}: 正在查詢第 {offset} 筆開始的資料...\")\n",
    "            try:\n",
    "                df = pd.read_sql(sql, engine)\n",
    "            except Exception as e:\n",
    "                print(f\"[查詢錯誤] {table_name} offset {offset}：{e}\")\n",
    "                break\n",
    "\n",
    "            if df.empty:\n",
    "                break\n",
    "\n",
    "            dfs.append(df)\n",
    "            offset += batch_size\n",
    "\n",
    "        if dfs:\n",
    "            final_df = pd.concat(dfs, ignore_index=True)\n",
    "            return table_name, final_df\n",
    "        else:\n",
    "            return table_name, pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[致命錯誤] {table_name} 查詢失敗：{e}\")\n",
    "        return table_name, None\n",
    "\n",
    "def query_multiple_tables_parallel(table_names, max_workers=4):\n",
    "    all_results = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(paginated_query, table) for table in table_names]\n",
    "        for future in as_completed(futures):\n",
    "            table_name, df = future.result()\n",
    "            all_results[table_name] = df\n",
    "\n",
    "    print(f\"\\n所有查詢完成，總耗時：{time.time() - start_time:.2f} 秒\")\n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables = [\"ana_table\", \"sponsor_view\", \"order_lines\", \"orders\"]#, \"league_map\", \"match_map\", \"team_map\"]\n",
    "    max_thread_workers = 6\n",
    "    results = query_multiple_tables_parallel(tables, max_workers=max_thread_workers)\n",
    "\n",
    "    for table, df in results.items():\n",
    "        if df is not None:\n",
    "            print(f\"{table}: {len(df)} rows\")\n",
    "        else:\n",
    "            print(f\"{table}: 查詢失敗\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
