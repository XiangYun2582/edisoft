{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff81ba2",
   "metadata": {},
   "source": [
    "## ç¶²è·¯çˆ¬èŸ²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e96121",
   "metadata": {},
   "source": [
    "### æŠ“æ¨™é¡Œ(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7352d308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thread-7 (fetch_url)] æˆåŠŸæŠ“å–ï¼šhttps://httpbin.org/html\n",
      "ğŸ“ https://httpbin.org/html çš„æ¨™é¡Œï¼šç„¡æ¨™é¡Œ\n",
      "[Thread-10 (fetch_url)] æˆåŠŸæŠ“å–ï¼šhttps://httpbin.org/user-agent\n",
      "ğŸ§¾ https://httpbin.org/user-agent çš„å…§å®¹ï¼š{\n",
      "  \"user-agent\": \"python-requests/2.32.4\"\n",
      "}\n",
      "...\n",
      "[Thread-9 (fetch_url)] æˆåŠŸæŠ“å–ï¼šhttps://httpbin.org/ip\n",
      "ğŸ§¾ https://httpbin.org/ip çš„å…§å®¹ï¼š{\n",
      "  \"origin\": \"211.20.1.45\"\n",
      "}\n",
      "...\n",
      "[Thread-8 (fetch_url)] æˆåŠŸæŠ“å–ï¼šhttps://httpbin.org/headers\n",
      "ğŸ§¾ https://httpbin.org/headers çš„å…§å®¹ï¼š{\n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept...\n",
      "[Thread-11 (fetch_url)] æˆåŠŸæŠ“å–ï¼šhttps://httpbin.org/get\n",
      "ğŸ§¾ https://httpbin.org/get çš„å…§å®¹ï¼š{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*...\n",
      "âœ… æ‰€æœ‰ä»»å‹™å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# æ¸¬è©¦ç¶²å€ï¼ˆhttpbin.org æ˜¯å¸¸è¦‹çš„æ¸¬è©¦ç¶²ç«™ï¼‰\n",
    "urls = [\n",
    "    \"https://httpbin.org/html\",\n",
    "    \"https://httpbin.org/headers\",\n",
    "    \"https://httpbin.org/ip\",\n",
    "    \"https://httpbin.org/user-agent\",\n",
    "    \"https://httpbin.org/get\"\n",
    "]\n",
    "\n",
    "# å·¥ä½œå‡½å¼ï¼šæŠ“å–ç¶²é æ¨™é¡Œæˆ–å…§å®¹\n",
    "def fetch_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        print(f\"[{threading.current_thread().name}] æˆåŠŸæŠ“å–ï¼š{url}\")\n",
    "        if \"html\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            title = soup.title.string if soup.title else \"ç„¡æ¨™é¡Œ\"\n",
    "            print(f\"ğŸ“ {url} çš„æ¨™é¡Œï¼š{title}\")\n",
    "        else:\n",
    "            print(f\"ğŸ§¾ {url} çš„å…§å®¹ï¼š{response.text[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{threading.current_thread().name}] âš ï¸ æŠ“å–å¤±æ•—ï¼š{url}\\nåŸå› ï¼š{e}\")\n",
    "\n",
    "def multi_thread_crawler(url_list):\n",
    "    threads = []\n",
    "\n",
    "    for url in url_list:\n",
    "        t = threading.Thread(target=fetch_url, args=(url,))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # ç­‰å¾…æ‰€æœ‰åŸ·è¡Œç·’çµæŸ\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    print(\"âœ… æ‰€æœ‰ä»»å‹™å®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    multi_thread_crawler(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a5802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ çˆ¬èŸ²çµæœï¼š\n",
      "https://httpbin.org/headers => ç„¡æ¨™é¡Œ\n",
      "https://httpbin.org/html => ç„¡æ¨™é¡Œ\n",
      "https://httpbin.org/user-agent => ç„¡æ¨™é¡Œ\n",
      "https://httpbin.org/ip => ç„¡æ¨™é¡Œ\n",
      "https://httpbin.org/get => ç„¡æ¨™é¡Œ\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue\n",
    "\n",
    "# ç¶²é åˆ—è¡¨ï¼ˆå¯æ›¿æ›ç‚ºä½ è¦çˆ¬çš„ç¶²ç«™ï¼‰\n",
    "urls = [\n",
    "    \"https://httpbin.org/html\",\n",
    "    \"https://httpbin.org/user-agent\",\n",
    "    \"https://httpbin.org/headers\",\n",
    "    \"https://httpbin.org/ip\",\n",
    "    \"https://httpbin.org/get\"\n",
    "]\n",
    "\n",
    "# å­˜æ”¾çˆ¬å–çµæœ\n",
    "results = []\n",
    "\n",
    "# å»ºç«‹ä½‡åˆ—\n",
    "url_queue = Queue()\n",
    "\n",
    "# æ”¾å…¥ç¶²å€\n",
    "for url in urls:\n",
    "    url_queue.put(url)\n",
    "\n",
    "# Lock ä¾†ä¿è­·å…±äº«è®Šæ•¸ï¼ˆå¦‚ resultsï¼‰\n",
    "lock = threading.Lock()\n",
    "\n",
    "# çˆ¬èŸ²åŸ·è¡Œç·’å‡½å¼\n",
    "def crawler_worker():\n",
    "    while not url_queue.empty():\n",
    "        url = url_queue.get()\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            title = soup.title.string.strip() if soup.title else \"ç„¡æ¨™é¡Œ\"\n",
    "        except Exception as e:\n",
    "            title = f\"æŠ“å–å¤±æ•—: {e}\"\n",
    "\n",
    "        with lock:\n",
    "            results.append((url, title))\n",
    "\n",
    "        url_queue.task_done()\n",
    "\n",
    "# å»ºç«‹å¤šå€‹åŸ·è¡Œç·’\n",
    "threads = []\n",
    "for i in range(3):  # å¯ä¾éœ€æ±‚èª¿æ•´åŸ·è¡Œç·’æ•¸\n",
    "    t = threading.Thread(target=crawler_worker, name=f\"Worker-{i+1}\")\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# ä½¿ç”¨ join ç­‰å¾…æ‰€æœ‰åŸ·è¡Œç·’å®Œæˆ\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"\\nğŸ“‹ çˆ¬èŸ²çµæœï¼š\")\n",
    "for url, title in results:\n",
    "    print(f\"{url} => {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc9260",
   "metadata": {},
   "source": [
    "### æŠ“æ¨™é¡Œ (lock 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbf9795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thread-12 (fetch_url)] æ­£åœ¨æŠ“å–: https://httpbin.org/html\n",
      "[Thread-13 (fetch_url)] æ­£åœ¨æŠ“å–: https://httpbin.org/user-agent\n",
      "[Thread-12 (fetch_url)] âœ… æˆåŠŸæŠ“å– https://httpbin.org/html -> ç„¡æ¨™é¡Œ\n",
      "[Thread-13 (fetch_url)] âœ… æˆåŠŸæŠ“å– https://httpbin.org/user-agent -> ç„¡æ¨™é¡Œ\n",
      "[Thread-14 (fetch_url)] æ­£åœ¨æŠ“å–: https://httpbin.org/headers\n",
      "[Thread-15 (fetch_url)] æ­£åœ¨æŠ“å–: https://httpbin.org/ip\n",
      "[Thread-15 (fetch_url)] âœ… æˆåŠŸæŠ“å– https://httpbin.org/ip -> ç„¡æ¨™é¡Œ\n",
      "[Thread-14 (fetch_url)] âœ… æˆåŠŸæŠ“å– https://httpbin.org/headers -> ç„¡æ¨™é¡Œ\n",
      "[Thread-16 (fetch_url)] æ­£åœ¨æŠ“å–: https://httpbin.org/get\n",
      "[Thread-16 (fetch_url)] âœ… æˆåŠŸæŠ“å– https://httpbin.org/get -> ç„¡æ¨™é¡Œ\n",
      "ğŸ‰ æ‰€æœ‰æŠ“å–å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# å»ºç«‹ Semaphoreï¼Œæœ€å¤šå…è¨±å…©å€‹åŸ·è¡Œç·’åŒæ™‚é€²å…¥\n",
    "sema = threading.Semaphore(2)\n",
    "\n",
    "urls = [\n",
    "    \"https://httpbin.org/html\",\n",
    "    \"https://httpbin.org/user-agent\",\n",
    "    \"https://httpbin.org/headers\",\n",
    "    \"https://httpbin.org/ip\",\n",
    "    \"https://httpbin.org/get\"\n",
    "]\n",
    "\n",
    "def fetch_url(url):\n",
    "    with sema:  # å–å¾—é€šè¡Œè­‰ï¼ˆåŒæ™‚æœ€å¤šå…©å€‹åŸ·è¡Œç·’é€šéï¼‰\n",
    "        print(f\"[{threading.current_thread().name}] æ­£åœ¨æŠ“å–: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            title = soup.title.string.strip() if soup.title else \"ç„¡æ¨™é¡Œ\"\n",
    "            print(f\"[{threading.current_thread().name}] âœ… æˆåŠŸæŠ“å– {url} -> {title}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{threading.current_thread().name}] âŒ éŒ¯èª¤: {url} -> {e}\")\n",
    "        time.sleep(1)  # æ¨¡æ“¬å»¶é²\n",
    "\n",
    "# å»ºç«‹æ‰€æœ‰çˆ¬èŸ²åŸ·è¡Œç·’\n",
    "threads = []\n",
    "for url in urls:\n",
    "    t = threading.Thread(target=fetch_url, args=(url,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# ç­‰å¾…æ‰€æœ‰åŸ·è¡Œç·’å®Œæˆ\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"ğŸ‰ æ‰€æœ‰æŠ“å–å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820c4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thread-17 (fetch_url)] ğŸš€ é–‹å§‹æŠ“å–: https://httpbin.org/html\n",
      "[Thread-18 (fetch_url)] ğŸš€ é–‹å§‹æŠ“å–: https://httpbin.org/user-agent\n",
      "[Thread-18 (fetch_url)] âœ… æŠ“å–å®Œæˆ: https://httpbin.org/user-agent â†’ ç„¡æ¨™é¡Œ\n",
      "[Thread-17 (fetch_url)] âœ… æŠ“å–å®Œæˆ: https://httpbin.org/html â†’ ç„¡æ¨™é¡Œ\n",
      "[Thread-19 (fetch_url)] ğŸš€ é–‹å§‹æŠ“å–: https://httpbin.org/headers\n",
      "[Thread-21 (fetch_url)] ğŸš€ é–‹å§‹æŠ“å–: https://httpbin.org/get\n",
      "[Thread-19 (fetch_url)] âœ… æŠ“å–å®Œæˆ: https://httpbin.org/headers â†’ ç„¡æ¨™é¡Œ\n",
      "[Thread-21 (fetch_url)] âœ… æŠ“å–å®Œæˆ: https://httpbin.org/get â†’ ç„¡æ¨™é¡Œ\n",
      "[Thread-20 (fetch_url)] ğŸš€ é–‹å§‹æŠ“å–: https://httpbin.org/ip\n",
      "[Thread-20 (fetch_url)] âœ… æŠ“å–å®Œæˆ: https://httpbin.org/ip â†’ ç„¡æ¨™é¡Œ\n",
      "ğŸ‰ æ‰€æœ‰ä»»å‹™å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# é–ï¼Œç”¨æ–¼ä¿è­· counter å…±äº«è³‡æº\n",
    "lock = threading.Lock()\n",
    "\n",
    "# æ¨¡æ“¬åŒæ™‚å¯é‹è¡Œçš„æœ€å¤§åŸ·è¡Œç·’æ•¸é‡\n",
    "MAX_RUNNING = 2\n",
    "current_running = 0  # ç›®å‰æ­£åœ¨åŸ·è¡Œçš„åŸ·è¡Œç·’æ•¸é‡\n",
    "\n",
    "urls = [\n",
    "    \"https://httpbin.org/html\",\n",
    "    \"https://httpbin.org/user-agent\",\n",
    "    \"https://httpbin.org/headers\",\n",
    "    \"https://httpbin.org/ip\",\n",
    "    \"https://httpbin.org/get\"\n",
    "]\n",
    "\n",
    "def fetch_url(url):\n",
    "    global current_running\n",
    "\n",
    "    # ç­‰å¾…ç›´åˆ°æœ‰ç©ºä½\n",
    "    while True:\n",
    "        with lock:\n",
    "            if current_running < MAX_RUNNING:\n",
    "                current_running += 1\n",
    "                break\n",
    "        time.sleep(0.1)  # ç¨å¾®ç­‰å¾…å†å˜—è©¦\n",
    "\n",
    "    try:\n",
    "        print(f\"[{threading.current_thread().name}] ğŸš€ é–‹å§‹æŠ“å–: {url}\")\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        title = soup.title.string.strip() if soup.title else \"ç„¡æ¨™é¡Œ\"\n",
    "        print(f\"[{threading.current_thread().name}] âœ… æŠ“å–å®Œæˆ: {url} â†’ {title}\")\n",
    "        time.sleep(1)  # æ¨¡æ“¬ä»»å‹™èŠ±è²»æ™‚é–“\n",
    "    except Exception as e:\n",
    "        print(f\"[{threading.current_thread().name}] âŒ æŠ“å–éŒ¯èª¤: {url} â†’ {e}\")\n",
    "    finally:\n",
    "        with lock:\n",
    "            current_running -= 1  # çµæŸæ™‚é‡‹æ”¾åé¡\n",
    "\n",
    "# å»ºç«‹èˆ‡å•Ÿå‹•æ‰€æœ‰åŸ·è¡Œç·’\n",
    "threads = []\n",
    "for url in urls:\n",
    "    t = threading.Thread(target=fetch_url, args=(url,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# ç­‰å¾…æ‰€æœ‰åŸ·è¡Œç·’å®Œæˆ\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"ğŸ‰ æ‰€æœ‰ä»»å‹™å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dfe96",
   "metadata": {},
   "source": [
    "### PPT cookie å•é¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "526eeb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTT å…«å¦æ¿æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      "[æ–°è] è‡ªç”±ç¯€ç ”è¨æœƒ å¤§ç´€å…ƒç¸½è£ç‰¹é‚€ç™¼è¡¨æ¼”è¬›\n",
      "[å•å¦] æ“æ ¸è‡ªé‡æ‰æ˜¯ç‹é“å°å—ï¼Ÿ\n",
      "Re: [å•å¦] æ—¥æœ¬ç‚ºä»€éº¼ä¸è·Ÿä¸­åœ‹æ–·äº¤?\n",
      "Re: [æ–°è] æ•™å¸«ç”„é¸ã€Œè©¦æ•™ã€æ‹¿20åˆ†æƒ¹è­° è€ƒç”Ÿç—›å“­2å¤©ï¼šæˆ‘éŒ¯åœ¨å“ªï¼Ÿ\n",
      "[æ–°è] ç¬¬6æ¬¡ï¼å½°åŒ–ã€Œé€™è·¯å£ã€åˆæœ‰è‡ªå°å®¢é¨ä¸Šåº‡\n",
      "Re: [æ–°è] æ›¾å‘½ä¸­è”¡è‹±æ–‡æœƒç•¶ç¸½çµ±ï¼å‘½ç†å¸«é è¨€æŸ¯æ–‡å“²\n",
      "Re: [æ–°è] é»ƒåœ‹æ˜ŒæŒ‘æˆ°å¸æ³•ï¼æ’­æ”¾åµè¨Šæœ±äºè™ã€Œç¤ºç¯„å¸¶\n",
      "[å•å¦] å˜¿å˜¿ é›æ¹¯ä¾†å›‰\n",
      "[å•å¦] è©é¨™é›†åœ˜æ˜¯ä¸æ˜¯æ ¹æœ¬ä¸ç”¨æ•‘?\n",
      "[å•å¦] ä»¥è‰²åˆ—æ‰“ä¼Šæœ—æ˜¯ç‚ºäº†ä¸»å‹•è‡ªè¡›å’Œäººé“å¹²æ¶‰ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# PTT å…«å¦æ¿ç¶²å€ï¼ˆéœ€è¦å¹´é½¡é©—è­‰ï¼‰\n",
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "\n",
    "# å»ºç«‹ session ç”¨ä¾†ä¿æŒ Cookie\n",
    "session = requests.Session()\n",
    "\n",
    "# æ¨¡æ“¬é€å‡ºã€Œæˆ‘å·²æ»¿18æ­²ã€çš„ Cookie\n",
    "session.cookies.set(\"over18\", \"1\")\n",
    "\n",
    "# é€å‡ºå¸¶ cookie çš„ GET è«‹æ±‚\n",
    "response = session.get(url)\n",
    "\n",
    "# ç¢ºèªæ˜¯å¦æˆåŠŸé€šéé©—è­‰ (PTTæœƒå›æ‡‰æ­£å¸¸é é¢)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    titles = soup.select(\".title a\")\n",
    "    print(\"PTT å…«å¦æ¿æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\")\n",
    "    for t in titles[:10]:  # åªåˆ—å‡ºå‰10ç¯‡\n",
    "        print(t.text)\n",
    "else:\n",
    "    print(\"ç„¡æ³•å­˜å–ï¼Œå¯èƒ½æ²’é€šéå¹´é½¡é©—è­‰æˆ–ç¶²ç«™ç•°å¸¸\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d0504",
   "metadata": {},
   "source": [
    "### PPT cookie å•é¡Œ (join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e030f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Thread-5 (fetch_ptt_board)] https://www.ptt.cc/bbs/Baseball/index.html æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      " - [æ–°è] MLBï¼ç´…è¥ªCampbellé­ä¸‹æ”¾3Aï¼å‰›ç²17.7å„„\n",
      " - Re: [è¨è«–] æ˜¨æ—¥å¯Œé‚¦è·‘å£˜\n",
      " - [åˆ†äº«] å±±æœ¬ç”±ä¼¸ç¬¬ä¸‰å±€11çƒ3K\n",
      " - [æ–°è] æ‚å°‡ä¸‹åŠå­£ä¸»é¡Œæ—¥ä¾†äº†ï¼é¦–åº¦è¯åæ—¥è·é—œ\n",
      " - [æ–°è] äº¤æ˜“å¾Œé¦–åº¦å°æ±º ç´…è¥ªæ•™é ­ï¼šå¾·å¼—æ–¯æœƒæƒ³æ‰“çˆ†\n",
      "\n",
      "[Thread-4 (fetch_ptt_board)] https://www.ptt.cc/bbs/Tech_Job/index.html æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      " - [æ–°è] è¯çˆ¾è¡—æ—¥å ±ï¼šä¸­åœ‹å·¥ç¨‹å¸«èµ´ç¬¬ä¸‰åœ‹ ç§Ÿç”¨è¼é”\n",
      " - [æ–°è] äºé¦¬éœåŸ·è¡Œé•·è³ˆè¥¿é–‹ç¬¬ä¸€æ§ ç¤ºè­¦ AI å°è‡´\n",
      " - [è¨è«–] å“ªäº›å…¬å¸æœ‰æä¾›å…è²»çš„å’–å•¡ï¼Ÿ\n",
      " - Re: [æ–°è] AIèª²å¤ªé›£ å¤§å­¸ç”Ÿçˆ†é€€é¸åœä¿®\n",
      " - [è¨è«–] å—éƒ¨æ˜¯ä¸æ˜¯æŠŠä¸»è¦ç”¢æ¥­å…¨éƒ¨æŠ¼å¯¶ä¸Šå°ç©äº†\n",
      "\n",
      "[Thread-3 (fetch_ptt_board)] https://www.ptt.cc/bbs/Gossiping/index.html æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      " - [å•å¦] 30ã€40æ­²æ²’å­˜æ¬¾çš„æ˜¯èŠ±å»å“ªï¼Ÿï¼Ÿ\n",
      " - Re: [å•å¦] åˆ°åº•ç‚ºä»€éº¼ä¸å°ä¸­å…±é–‹ç«ï¼Ÿï¼Ÿï¼Ÿ\n",
      " - Re: [æ–°è] æ­»å›šé„­æ­¦æ¾åˆ¤æ­»å®šè®20å¹´ è²è«‹å†å¯©ä¸¦åœæ­¢\n",
      " - [å•å¦] æœˆè–ª48Kç®¡ç†è·åœ¨é«˜é›„PRå¤§æ¦‚å¤šå°‘ï¼Ÿ\n",
      " - [å•å¦] ç§»å·¥å¦¹å­éƒ½ç”¨åŒç‰Œæ´—è¡£ç²¾æ´—é«®ç²¾æ²æµ´ä¹³?\n",
      "\n",
      "æ‰€æœ‰æ¿çš„æ–‡ç« æ¨™é¡ŒæŠ“å–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# è¦çˆ¬çš„ PTT çœ‹æ¿æˆ–é é¢åˆ—è¡¨ï¼ˆç¤ºç¯„å¹¾å€‹æ¿ï¼‰\n",
    "urls = [\n",
    "    \"https://www.ptt.cc/bbs/Gossiping/index.html\",\n",
    "    \"https://www.ptt.cc/bbs/Tech_Job/index.html\",\n",
    "    \"https://www.ptt.cc/bbs/Baseball/index.html\"\n",
    "]\n",
    "\n",
    "def fetch_ptt_board(url):\n",
    "    # æ¯å€‹åŸ·è¡Œç·’éƒ½å»ºç«‹ç¨ç«‹çš„ session\n",
    "    session = requests.Session()\n",
    "    # æ¨¡æ“¬å·²é€šé 18 æ­²é©—è­‰ Cookie\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "\n",
    "    try:\n",
    "        res = session.get(url, timeout=10)\n",
    "        if res.status_code == 200:\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            titles = soup.select(\".title a\")\n",
    "            print(f\"\\n[{threading.current_thread().name}] {url} æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\")\n",
    "            for t in titles[:5]:  # é¡¯ç¤ºå‰5ç¯‡æ¨™é¡Œ\n",
    "                print(\" -\", t.text)\n",
    "        else:\n",
    "            print(f\"[{threading.current_thread().name}] ç„¡æ³•å­˜å– {url}ï¼ŒHTTPç‹€æ…‹ç¢¼ï¼š{res.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{threading.current_thread().name}] ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "\n",
    "threads = []\n",
    "\n",
    "# å»ºç«‹ä¸¦å•Ÿå‹•åŸ·è¡Œç·’\n",
    "for url in urls:\n",
    "    t = threading.Thread(target=fetch_ptt_board, args=(url,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# ç­‰å¾…æ‰€æœ‰åŸ·è¡Œç·’çµæŸ\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"\\næ‰€æœ‰æ¿çš„æ–‡ç« æ¨™é¡ŒæŠ“å–å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83497e7d",
   "metadata": {},
   "source": [
    "### PPT cookie å•é¡Œ (å–®åŸ·è¡Œç·’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9d09ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å¤šåŸ·è¡Œç·’çˆ¬å– ===\n",
      "[Thread-31 (fetch_ptt_board)] https://www.ptt.cc/bbs/Gossiping/index.html æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      " - [å•å¦] è¬›åˆ°BMIç‚ºä»€éº¼å°±æœƒæœ‰äººç”Ÿæ°£ï¼Ÿ\n",
      " - [å•å¦] ç‚ºä»€éº¼éŠ€æ¨“éƒ½å–œæ­¡æ”¶ç¾é‡‘å•Šï¼Ÿ\n",
      " - [å•å¦] windows ä»‹é¢æ€éº¼é€™éº¼é†œ?\n",
      "[Thread-32 (fetch_ptt_board)] https://www.ptt.cc/bbs/Tech_Job/index.html æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      " - [æ–°è] è¯çˆ¾è¡—æ—¥å ±ï¼šä¸­åœ‹å·¥ç¨‹å¸«èµ´ç¬¬ä¸‰åœ‹ ç§Ÿç”¨è¼é”\n",
      " - [æ–°è] äºé¦¬éœåŸ·è¡Œé•·è³ˆè¥¿é–‹ç¬¬ä¸€æ§ ç¤ºè­¦ AI å°è‡´\n",
      " - [è¨è«–] å“ªäº›å…¬å¸æœ‰æä¾›å…è²»çš„å’–å•¡ï¼Ÿ\n",
      "[Thread-33 (fetch_ptt_board)] https://www.ptt.cc/bbs/Baseball/index.html æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      " - [æƒ…å ±] é“å¥‡éšŠæ‹’çµ•ICEæ¢å“¡é€²å…¥çƒå ´\n",
      " - [åˆ†äº«] Tony Gonsolin ç§»å¾€60å¤©å‚·å…µåå–®\n",
      " - [åˆ†äº«] ä»Šæ—¥Logan Webb 7.0IP/1R/9K\n",
      "\n",
      "å¤šåŸ·è¡Œç·’èŠ±è²»æ™‚é–“: 0.65 ç§’\n",
      "\n",
      "=== å–®åŸ·è¡Œç·’çˆ¬å– ===\n",
      "[MainThread] https://www.ptt.cc/bbs/Gossiping/index.html æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      " - [å•å¦] è¬›åˆ°BMIç‚ºä»€éº¼å°±æœƒæœ‰äººç”Ÿæ°£ï¼Ÿ\n",
      " - [å•å¦] ç‚ºä»€éº¼éŠ€æ¨“éƒ½å–œæ­¡æ”¶ç¾é‡‘å•Šï¼Ÿ\n",
      " - [å•å¦] windows ä»‹é¢æ€éº¼é€™éº¼é†œ?\n",
      "[MainThread] https://www.ptt.cc/bbs/Tech_Job/index.html æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      " - [æ–°è] è¯çˆ¾è¡—æ—¥å ±ï¼šä¸­åœ‹å·¥ç¨‹å¸«èµ´ç¬¬ä¸‰åœ‹ ç§Ÿç”¨è¼é”\n",
      " - [æ–°è] äºé¦¬éœåŸ·è¡Œé•·è³ˆè¥¿é–‹ç¬¬ä¸€æ§ ç¤ºè­¦ AI å°è‡´\n",
      " - [è¨è«–] å“ªäº›å…¬å¸æœ‰æä¾›å…è²»çš„å’–å•¡ï¼Ÿ\n",
      "[MainThread] https://www.ptt.cc/bbs/Baseball/index.html æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\n",
      " - [æƒ…å ±] é“å¥‡éšŠæ‹’çµ•ICEæ¢å“¡é€²å…¥çƒå ´\n",
      " - [åˆ†äº«] Tony Gonsolin ç§»å¾€60å¤©å‚·å…µåå–®\n",
      " - [åˆ†äº«] ä»Šæ—¥Logan Webb 7.0IP/1R/9K\n",
      "\n",
      "å–®åŸ·è¡Œç·’èŠ±è²»æ™‚é–“: 1.87 ç§’\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "urls = [\n",
    "    \"https://www.ptt.cc/bbs/Gossiping/index.html\",\n",
    "    \"https://www.ptt.cc/bbs/Tech_Job/index.html\",\n",
    "    \"https://www.ptt.cc/bbs/Baseball/index.html\"\n",
    "]\n",
    "\n",
    "def fetch_ptt_board(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        titles = soup.select(\".title a\")\n",
    "        print(f\"[{threading.current_thread().name}] {url} æœ€æ–°æ–‡ç« æ¨™é¡Œï¼š\")\n",
    "        for t in titles[:3]:\n",
    "            print(\" -\", t.text)\n",
    "    else:\n",
    "        print(f\"[{threading.current_thread().name}] ç„¡æ³•å­˜å– {url}\")\n",
    "\n",
    "def multi_thread_crawl():\n",
    "    threads = []\n",
    "    start = time.time()\n",
    "\n",
    "    for url in urls:\n",
    "        t = threading.Thread(target=fetch_ptt_board, args=(url,))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\nå¤šåŸ·è¡Œç·’èŠ±è²»æ™‚é–“: {end - start:.2f} ç§’\")\n",
    "\n",
    "def single_thread_crawl():\n",
    "    start = time.time()\n",
    "    for url in urls:\n",
    "        fetch_ptt_board(url)\n",
    "    end = time.time()\n",
    "    print(f\"\\nå–®åŸ·è¡Œç·’èŠ±è²»æ™‚é–“: {end - start:.2f} ç§’\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== å¤šåŸ·è¡Œç·’çˆ¬å– ===\")\n",
    "    multi_thread_crawl()\n",
    "    print(\"\\n=== å–®åŸ·è¡Œç·’çˆ¬å– ===\")\n",
    "    single_thread_crawl()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0265ad4",
   "metadata": {},
   "source": [
    "### PPT cookie å•é¡Œ + å…§å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da156775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] [å•å¦] è¬›åˆ°BMIç‚ºä»€éº¼å°±æœƒæœ‰äººç”Ÿæ°£ï¼Ÿ\n",
      "\n",
      "[æ–‡ç« å…§å®¹] https://www.ptt.cc/bbs/Gossiping/M.1750389849.A.B44.htmlï¼š\n",
      "æ¯æ¬¡è¬›åˆ°BMIéƒ½æœƒæœ‰äººæ°£å¾—è·³è…³\n",
      "\n",
      "èªªBMIæ²’ç”¨ æ³¡èŠ™äººçœ‹ä¸å‡ºä¾†\n",
      "\n",
      "å¯æ˜¯åˆ°ç¾åœ¨é†«å­¸ä»ç„¶æ˜¯ç”¨BMIä¾†åˆ¤æ–·è‚¥èƒ– éé‡æ¨™æº–\n",
      "\n",
      "è€Œä¸”å°ç£å·¨å·¨ä¹Ÿæ²’é€™éº¼å¤š\n",
      "\n",
      "\n",
      "\n",
      "ç”¨\"é«”è„‚\"ä¾†ç•¶æ¨™æº–çš„è©±\n",
      "\n",
      "19æ­²ä»¥ä¸Šæˆäººä¸­ï¼Œéé‡ä»¥ä¸Šäººå£ä½”å…¨é«”äººå£ä¹‹ç™¾åˆ†æ¯”\n",
      "*ç”·æ€§é«”è„‚è‚ªâ‰§25%ï¼Œå¥³æ€§é«”è„‚è‚ªâ‰§30%\n",
      "https://i.imgur.com/SCs7smX.png\n",
      "ä¾æ“šä¸Šè¿°é«”è„‚ç‡æ¨™æº–\n",
      "ç”·æ€§å¹´è¼•çš„å…­æˆéé‡\n",
      "ä¸­è€å¹´å…«æˆ\n",
      "\n",
      "å¥³æ€§å¹´è¼•çš„å…«æˆéé‡\n",
      "ä¸­è€å¹´ä¹æˆ\n",
      "\n",
      "å¥½åƒæ¯”ç”¨BMIæ›´æ…˜\n",
      "\n",
      "\n",
      "BMIæ˜¯ä¸€å¡Šé®ç¾å¸ƒå—\n",
      "\n",
      "æœ‰æ²’æœ‰å…«å¦ï¼Ÿ\n",
      "\n",
      "--\n",
      "â€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc), ä¾†è‡ª: 220.128.198.204 (è‡ºç£)\n",
      "â€» æ–‡ç« ç¶²å€:\n",
      "...\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] [å•å¦] ç‚ºä»€éº¼éŠ€æ¨“éƒ½å–œæ­¡æ”¶ç¾é‡‘å•Šï¼Ÿ\n",
      "\n",
      "[æ–‡ç« å…§å®¹] https://www.ptt.cc/bbs/Gossiping/M.1750389850.A.A12.htmlï¼š\n",
      "å»éŠ€æ¨“è³¼è²·é‡‘é£¾çš„æ™‚å€™ï¼Œæƒ³ç”¨ä¿¡ç”¨å¡çµå¸³ï¼Œ\n",
      "å»è¢«è€é—†å‘ŠçŸ¥ï¼š\n",
      "å¦‚æœåˆ·å¡è¦è¢«æ”¶3%ï¼Œ2è¬è¦æ”¶600ï¼Œ\n",
      "ç¾é‡‘ä»˜æ¬¾å°æˆ‘æ¯”è¼ƒæœ‰åˆ©\n",
      "\n",
      "ç‚ºäº†çœä¸‹å°éŒ¢ä¹Ÿæ˜¯å»æé ˜äº†ï¼Œ\n",
      "åˆ·å¡çš„å›æ‰£ä¹Ÿæ¯”ä¸ä¸Š3%å°±æ˜¯äº†\n",
      "\n",
      "ç¾é‡‘èƒ½è®“ä»–å€‘é€ƒæ¼å¤šå°‘ç¨…å•Šï¼Ÿ\n",
      "5%çš„ç‡Ÿæ¥­ç¨…ï¼Œé‚„æ˜¯å¤šå°‘ç‡Ÿæ‰€ç¨…ï¼Ÿ\n",
      "\n",
      "--\n",
      "â€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc), ä¾†è‡ª: 42.76.160.5 (è‡ºç£)\n",
      "â€» æ–‡ç« ç¶²å€:\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1750389850.A.A12.html...\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] [å•å¦] windows ä»‹é¢æ€éº¼é€™éº¼é†œ?\n",
      "\n",
      "[æ–‡ç« å…§å®¹] https://www.ptt.cc/bbs/Gossiping/M.1750389864.A.EC1.htmlï¼š\n",
      "mac osé€™ç¨®çš„ä¸èªª ç•¢ç«Ÿæœå­ç¾å­¸é‚„æ˜¯åœ¨ç·šçš„\n",
      "\n",
      "ä½†æ˜¯éš¨ä¾¿ä¸€å€‹linuxç™¼è¡Œç‰ˆ UBUNTU FEDORA\n",
      "\n",
      "é‚£å€‹UI å­—é«”éƒ½å±Œæ‰“WINDOWS\n",
      "\n",
      "è»Ÿè»Ÿå…§éƒ¨æ²’æœ‰äººæ‰å—?è¨­è¨ˆä¸€å€‹å¥½çœ‹çš„æ¡Œé¢ç’°å¢ƒå¾ˆé›£?\n",
      "\n",
      "ç‚ºä»€éº¼WINDOWSçœ‹èµ·ä¾†é€™éº¼é†œ?æœ‰ç„¡å…«å¦?\n",
      "\n",
      "--\n",
      "å¦‚ä½•å˜´ç ²\n",
      "åé§å°æ–¹çš„é‡é»â”€â”€â—¢â—£\n",
      "â–ˆ\n",
      "ç¢ºå¯¦æŒ‡å‡ºäººå®¶è«–é»çš„éŒ¯èª¤æ€§\n",
      "ÏˆQSWEET\n",
      "â”‚\n",
      "ï¼  â—\n",
      "é§æ–¥\n",
      "â”€â”€â”€â”€â”€â”€\n",
      "â—¢\n",
      "â—£\n",
      "â–ˆ\n",
      "ç”¨å¼•è¨€æŒ‡å‡ºå°æ–¹éŒ¯èª¤æˆ–çŸ›ç›¾çš„åœ°æ–¹\n",
      "(\n",
      "â–ˆ\n",
      "å„ªè³ªè«–æ–‡ï¼‰\n",
      "åœ¨å˜´ç ²ç‹\n",
      "ç›¸åçš„è§€é»\n",
      "â”€â”€\n",
      "â—¢â–ˆâ–ˆâ–ˆâ–ˆâ—£\n",
      "â–ˆ\n",
      "åˆ—å‡ºç›¸åçš„è«–é»ä¸¦ä»¥äº‹å¯¦ç•¶è­‰æ“š\n",
      "(\n",
      "â–ˆ\n",
      "è¾¯è«–ç¤¾ï¼‰\n",
      "æ‡‰è©²å‡ºç¾\n",
      "âŠ™\n",
      "çŸ›ç›¾\n",
      "â”€â”€â”€â”€\n",
      "â—¢â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ—£\n",
      "â–ˆ\n",
      "åˆ—å‡ºç›¸åçš„è«–é»ä½†ä¸åŠ ä»¥...\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] Re: [å•å¦] éŸ“åœ‹æ­å·´ 1992å¹´æ‰è·Ÿä¸­è¯æ°‘åœ‹æ–·äº¤ï¼Ÿ\n",
      "\n",
      "[æ–‡ç« å…§å®¹] https://www.ptt.cc/bbs/Gossiping/M.1750389867.A.7B7.htmlï¼š\n",
      "â€» å¼•è¿°ã€Šseabox (å‘‚é›…ç­‘)ã€‹ä¹‹éŠ˜è¨€ï¼š\n",
      ": ç¾å¸è·Ÿæ—¥å¯‡\n",
      ": é‚„æœ‰ç¾åœ‹çš„æ­æ´²èµ°ç‹—å€‘\n",
      ": éƒ½æ—©æ—©è·Ÿæˆ‘å€‘ä¸­è¯æ°‘åœ‹ï¼ˆä¸­åœ‹ï¼‰æ–·äº¤\n",
      ": éŸ“åœ‹æ­å·´\n",
      ": ç›´åˆ°1992å¹´éƒ½é‚„æ‰¿èªä¸­è¯æ°‘åœ‹å°±æ˜¯ä¸­åœ‹\n",
      ": ç‚ºä»€éº¼éŸ“åœ‹æ­å·´é‚£éº¼ä¹…æ‰æ–·äº¤å‘¢\n",
      "å¦‚æœä½ æœ‰è·Ÿäºæ´²å„åœ‹äººäº¤å¾€\n",
      "\n",
      "ä½ å°±æœƒç™¼ç¾\n",
      "\n",
      "é›–ç„¶ä¸€ç›´å–Šå°æ—¥å‹å¥½ ä½†æ˜¯é‚£æ›´å¤šçš„åªæ˜¯å°ç£äººçš„ä¸€å»‚æƒ…é¡˜\n",
      "\n",
      "\n",
      "å…¶å¯¦éŸ“åœ‹äººå€‹æ€§è·Ÿå°ç£äººæ˜¯æ¯”è¼ƒåƒçš„\n",
      "éŸ“åœ‹äººæ˜¯çœŸçš„æŠŠä½ ç•¶å…„å¼Ÿé‚£ç¨®\n",
      "\n",
      "\n",
      "-----\n",
      "Sent from JPTT on my Xiaomi 2201117SY.\n",
      "\n",
      "--\n",
      "â€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc), ä¾†è‡ª: 49.217.62.212 (è‡ºç£)\n",
      "â€» æ–‡ç« ç¶²å€:\n",
      "https...\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] [å•å¦] ä¸­è¯æ°‘åœ‹è‡ºç£é‚£éº¼æ£’æ€æ²’æœ‹å‹ï¼Ÿ\n",
      "\n",
      "[æ–‡ç« å…§å®¹] https://www.ptt.cc/bbs/Gossiping/M.1750389868.A.D73.htmlï¼š\n",
      "å¦‚é¡Œå•¦\n",
      "ä¸­è¯æ°‘åœ‹è‡ºç£\n",
      "æœ‰å°ç©é›»\n",
      "æœ‰è¶…å¥½åƒçš„å°åƒ\n",
      "æœ‰è¶…å–„è‰¯çš„äºº\n",
      "è¶…åå…±\n",
      "æ€éº¼æ²’æœ‰åœ‹å®¶\n",
      "è¦è·Ÿæˆ‘å€‘åšæœ‹å‹\n",
      "åªæœƒè¬›å‹å¥½å ´é¢è©±\n",
      "æœ‰æ²’æœ‰å¦\n",
      "QQ\n",
      "\n",
      "--\n",
      "â€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc), ä¾†è‡ª: 49.218.209.251 (è‡ºç£)\n",
      "â€» æ–‡ç« ç¶²å€:\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1750389868.A.D73.html...\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "ç¸½èŠ±è²»æ™‚é–“: 3.67 ç§’\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://www.ptt.cc\"\n",
    "BOARD_URL = BASE_URL + \"/bbs/Gossiping/index.html\"\n",
    "\n",
    "# é€²å…¥æ–‡ç« çˆ¬å…§æ–‡çš„å‡½å¼ï¼Œé€™è£¡åŒæ­¥åŸ·è¡Œï¼Œæœƒç­‰å¾…æ­¤å‡½å¼å®Œæˆå¾Œæ‰ç¹¼çºŒ\n",
    "def fetch_article_content(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        main_content = soup.find(id=\"main-content\")\n",
    "        # ç§»é™¤æ–‡ç« å…§çš„ç•™è¨€å€ç­‰ä¸å¿…è¦å…§å®¹\n",
    "        for tag in main_content.find_all(['div', 'span', 'a'], class_=['article-metaline', 'article-metaline-right', 'push']):\n",
    "            tag.decompose()\n",
    "        text = main_content.get_text(strip=True, separator=\"\\n\")\n",
    "        print(f\"\\n[æ–‡ç« å…§å®¹] {url}ï¼š\\n{text[:300]}...\")  # åªå°å‰300å­—\n",
    "    else:\n",
    "        print(f\"ç„¡æ³•å–å¾—æ–‡ç« å…§å®¹ï¼š{url}\")\n",
    "\n",
    "def fetch_board_articles():\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(BOARD_URL, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        print(\"ç„¡æ³•å–å¾—çœ‹æ¿é é¢\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = soup.select(\".title a\")\n",
    "\n",
    "    for a in articles[:5]:  # å…ˆæŠ“å‰5ç¯‡ç¤ºç¯„\n",
    "        title = a.text\n",
    "        href = a['href']\n",
    "        article_url = BASE_URL + href\n",
    "        print(f\"\\n[æ–‡ç« æ¨™é¡Œ] {title}\")\n",
    "        \n",
    "        # å»ºç«‹åŸ·è¡Œç·’å»çˆ¬æ–‡ç« å…§æ–‡\n",
    "        t = threading.Thread(target=fetch_article_content, args=(article_url,))\n",
    "        t.start()\n",
    "        # ç”¨ join ç­‰å¾…æ­¤æ–‡ç« å…§æ–‡çˆ¬å®Œï¼Œå†ç¹¼çºŒä¸‹ä¸€ç¯‡\n",
    "        t.join()\n",
    "        print(\"=== æ–‡ç« æŠ“å–å®Œæˆ ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    fetch_board_articles()\n",
    "    print(f\"\\nç¸½èŠ±è²»æ™‚é–“: {time.time() - start_time:.2f} ç§’\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302ed9f",
   "metadata": {},
   "source": [
    "### PPT + çœ‹æ¿ + å…§å®¹ (lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70d89475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] [å•å¦] è¬›åˆ°BMIç‚ºä»€éº¼å°±æœƒæœ‰äººç”Ÿæ°£ï¼Ÿ\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] [å•å¦] ç‚ºä»€éº¼éŠ€æ¨“éƒ½å–œæ­¡æ”¶ç¾é‡‘å•Šï¼Ÿ\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] [å•å¦] windows ä»‹é¢æ€éº¼é€™éº¼é†œ?\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] Re: [å•å¦] éŸ“åœ‹æ­å·´ 1992å¹´æ‰è·Ÿä¸­è¯æ°‘åœ‹æ–·äº¤ï¼Ÿ\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "[æ–‡ç« æ¨™é¡Œ] [å•å¦] ä¸­è¯æ°‘åœ‹è‡ºç£é‚£éº¼æ£’æ€æ²’æœ‹å‹ï¼Ÿ\n",
      "=== æ–‡ç« æŠ“å–å®Œæˆ ===\n",
      "\n",
      "ç¸½èŠ±è²»æ™‚é–“: 3.64 ç§’\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "BASE_URL = \"https://www.ptt.cc\"\n",
    "BOARD_URL = BASE_URL + \"/bbs/Gossiping/index.html\"\n",
    "\n",
    "results = []       # å…¨éƒ¨æ–‡ç« è³‡æ–™æœƒå­˜é€™è£¡\n",
    "results_lock = threading.Lock()  # ä¿è­· results è³‡æ–™çµæ§‹çš„é–\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        main_content = soup.find(id=\"main-content\")\n",
    "        for tag in main_content.find_all(['div', 'span', 'a'], class_=['article-metaline', 'article-metaline-right', 'push']):\n",
    "            tag.decompose()\n",
    "        text = main_content.get_text(strip=True, separator=\"\\n\")\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def fetch_board_articles():\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(BOARD_URL, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        print(\"ç„¡æ³•å–å¾—çœ‹æ¿é é¢\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = soup.select(\".title a\")\n",
    "\n",
    "    for a in articles[:5]:  # å…ˆæŠ“å‰5ç¯‡ç¤ºç¯„\n",
    "        title = a.text\n",
    "        href = a['href']\n",
    "        article_url = BASE_URL + href\n",
    "        print(f\"\\n[æ–‡ç« æ¨™é¡Œ] {title}\")\n",
    "\n",
    "        # ç”¨åŸ·è¡Œç·’æŠ“æ–‡ç« å…§æ–‡ä¸¦å–å¾—çµæœ\n",
    "        content = None\n",
    "\n",
    "        def thread_job():\n",
    "            nonlocal content\n",
    "            content = fetch_article_content(article_url)\n",
    "\n",
    "        t = threading.Thread(target=thread_job)\n",
    "        t.start()\n",
    "        t.join()\n",
    "\n",
    "        print(\"=== æ–‡ç« æŠ“å–å®Œæˆ ===\")\n",
    "\n",
    "        # ç”¨é–ä¿è­· results çš„ä¿®æ”¹\n",
    "        with results_lock:\n",
    "            results.append({\n",
    "                \"title\": title,\n",
    "                \"url\": article_url,\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    fetch_board_articles()\n",
    "    print(f\"\\nç¸½èŠ±è²»æ™‚é–“: {time.time() - start_time:.2f} ç§’\")\n",
    "\n",
    "    # å­˜æˆ JSON æª”æ¡ˆ\n",
    "    # with open(\"ptt_gossiping_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    # print(\"è³‡æ–™å·²å­˜æˆ ptt_gossiping_articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72b42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '[å•å¦] è¬›åˆ°BMIç‚ºä»€éº¼å°±æœƒæœ‰äººç”Ÿæ°£ï¼Ÿ',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389849.A.B44.html',\n",
       "  'content': 'æ¯æ¬¡è¬›åˆ°BMIéƒ½æœƒæœ‰äººæ°£å¾—è·³è…³\\n\\nèªªBMIæ²’ç”¨ æ³¡èŠ™äººçœ‹ä¸å‡ºä¾†\\n\\nå¯æ˜¯åˆ°ç¾åœ¨é†«å­¸ä»ç„¶æ˜¯ç”¨BMIä¾†åˆ¤æ–·è‚¥èƒ– éé‡æ¨™æº–\\n\\nè€Œä¸”å°ç£å·¨å·¨ä¹Ÿæ²’é€™éº¼å¤š\\n\\n\\n\\nç”¨\"é«”è„‚\"ä¾†ç•¶æ¨™æº–çš„è©±\\n\\n19æ­²ä»¥ä¸Šæˆäººä¸­ï¼Œéé‡ä»¥ä¸Šäººå£ä½”å…¨é«”äººå£ä¹‹ç™¾åˆ†æ¯”\\n*ç”·æ€§é«”è„‚è‚ªâ‰§25%ï¼Œå¥³æ€§é«”è„‚è‚ªâ‰§30%\\nhttps://i.imgur.com/SCs7smX.png\\nä¾æ“šä¸Šè¿°é«”è„‚ç‡æ¨™æº–\\nç”·æ€§å¹´è¼•çš„å…­æˆéé‡\\nä¸­è€å¹´å…«æˆ\\n\\nå¥³æ€§å¹´è¼•çš„å…«æˆéé‡\\nä¸­è€å¹´ä¹æˆ\\n\\nå¥½åƒæ¯”ç”¨BMIæ›´æ…˜\\n\\n\\nBMIæ˜¯ä¸€å¡Šé®ç¾å¸ƒå—\\n\\næœ‰æ²’æœ‰å…«å¦ï¼Ÿ\\n\\n--\\nâ€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc), ä¾†è‡ª: 220.128.198.204 (è‡ºç£)\\nâ€» æ–‡ç« ç¶²å€:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389849.A.B44.html'},\n",
       " {'title': '[å•å¦] ç‚ºä»€éº¼éŠ€æ¨“éƒ½å–œæ­¡æ”¶ç¾é‡‘å•Šï¼Ÿ',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389850.A.A12.html',\n",
       "  'content': 'å»éŠ€æ¨“è³¼è²·é‡‘é£¾çš„æ™‚å€™ï¼Œæƒ³ç”¨ä¿¡ç”¨å¡çµå¸³ï¼Œ\\nå»è¢«è€é—†å‘ŠçŸ¥ï¼š\\nå¦‚æœåˆ·å¡è¦è¢«æ”¶3%ï¼Œ2è¬è¦æ”¶600ï¼Œ\\nç¾é‡‘ä»˜æ¬¾å°æˆ‘æ¯”è¼ƒæœ‰åˆ©\\n\\nç‚ºäº†çœä¸‹å°éŒ¢ä¹Ÿæ˜¯å»æé ˜äº†ï¼Œ\\nåˆ·å¡çš„å›æ‰£ä¹Ÿæ¯”ä¸ä¸Š3%å°±æ˜¯äº†\\n\\nç¾é‡‘èƒ½è®“ä»–å€‘é€ƒæ¼å¤šå°‘ç¨…å•Šï¼Ÿ\\n5%çš„ç‡Ÿæ¥­ç¨…ï¼Œé‚„æ˜¯å¤šå°‘ç‡Ÿæ‰€ç¨…ï¼Ÿ\\n\\n--\\nâ€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc), ä¾†è‡ª: 42.76.160.5 (è‡ºç£)\\nâ€» æ–‡ç« ç¶²å€:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389850.A.A12.html'},\n",
       " {'title': '[å•å¦] windows ä»‹é¢æ€éº¼é€™éº¼é†œ?',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389864.A.EC1.html',\n",
       "  'content': 'mac osé€™ç¨®çš„ä¸èªª ç•¢ç«Ÿæœå­ç¾å­¸é‚„æ˜¯åœ¨ç·šçš„\\n\\nä½†æ˜¯éš¨ä¾¿ä¸€å€‹linuxç™¼è¡Œç‰ˆ UBUNTU FEDORA\\n\\né‚£å€‹UI å­—é«”éƒ½å±Œæ‰“WINDOWS\\n\\nè»Ÿè»Ÿå…§éƒ¨æ²’æœ‰äººæ‰å—?è¨­è¨ˆä¸€å€‹å¥½çœ‹çš„æ¡Œé¢ç’°å¢ƒå¾ˆé›£?\\n\\nç‚ºä»€éº¼WINDOWSçœ‹èµ·ä¾†é€™éº¼é†œ?æœ‰ç„¡å…«å¦?\\n\\n--\\nå¦‚ä½•å˜´ç ²\\nåé§å°æ–¹çš„é‡é»â”€â”€â—¢â—£\\nâ–ˆ\\nç¢ºå¯¦æŒ‡å‡ºäººå®¶è«–é»çš„éŒ¯èª¤æ€§\\nÏˆQSWEET\\nâ”‚\\nï¼  â—\\né§æ–¥\\nâ”€â”€â”€â”€â”€â”€\\nâ—¢\\nâ—£\\nâ–ˆ\\nç”¨å¼•è¨€æŒ‡å‡ºå°æ–¹éŒ¯èª¤æˆ–çŸ›ç›¾çš„åœ°æ–¹\\n(\\nâ–ˆ\\nå„ªè³ªè«–æ–‡ï¼‰\\nåœ¨å˜´ç ²ç‹\\nç›¸åçš„è§€é»\\nâ”€â”€\\nâ—¢â–ˆâ–ˆâ–ˆâ–ˆâ—£\\nâ–ˆ\\nåˆ—å‡ºç›¸åçš„è«–é»ä¸¦ä»¥äº‹å¯¦ç•¶è­‰æ“š\\n(\\nâ–ˆ\\nè¾¯è«–ç¤¾ï¼‰\\næ‡‰è©²å‡ºç¾\\nâŠ™\\nçŸ›ç›¾\\nâ”€â”€â”€â”€\\nâ—¢â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ—£\\nâ–ˆ\\nåˆ—å‡ºç›¸åçš„è«–é»ä½†ä¸åŠ ä»¥è­‰å¯¦\\n(\\nâ–ˆ\\nâ–ˆ\\nè«–å£‡)\\nçš„å…ƒç´ \\næ”»æ“Šæ…‹åº¦\\nâ”€\\nâ—¢â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ—£\\nâ–ˆ\\nè³ªç–‘å°æ–¹çš„æ…‹åº¦å’Œå£æ°£\\n(\\nâ–ˆ\\nâ–ˆ\\nåŒ¿åç‰ˆ)\\näººèº«æ”»æ“Š\\nâ†˜\\nåè¦‹\\nâ†—\\nâ–„\\nâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„\\nâ–ˆ\\næ”»æ“Šèº«ä»½å’Œèƒ½è€\\nâ–ˆ\\nå¹¹ä½ å¨˜\\n(\\nâ–ˆ\\nå°æœ‹å‹)\\n--\\nâ€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc), ä¾†è‡ª: 36.228.227.41 (è‡ºç£)\\nâ€» æ–‡ç« ç¶²å€:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389864.A.EC1.html'},\n",
       " {'title': 'Re: [å•å¦] éŸ“åœ‹æ­å·´ 1992å¹´æ‰è·Ÿä¸­è¯æ°‘åœ‹æ–·äº¤ï¼Ÿ',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389867.A.7B7.html',\n",
       "  'content': 'â€» å¼•è¿°ã€Šseabox (å‘‚é›…ç­‘)ã€‹ä¹‹éŠ˜è¨€ï¼š\\n: ç¾å¸è·Ÿæ—¥å¯‡\\n: é‚„æœ‰ç¾åœ‹çš„æ­æ´²èµ°ç‹—å€‘\\n: éƒ½æ—©æ—©è·Ÿæˆ‘å€‘ä¸­è¯æ°‘åœ‹ï¼ˆä¸­åœ‹ï¼‰æ–·äº¤\\n: éŸ“åœ‹æ­å·´\\n: ç›´åˆ°1992å¹´éƒ½é‚„æ‰¿èªä¸­è¯æ°‘åœ‹å°±æ˜¯ä¸­åœ‹\\n: ç‚ºä»€éº¼éŸ“åœ‹æ­å·´é‚£éº¼ä¹…æ‰æ–·äº¤å‘¢\\nå¦‚æœä½ æœ‰è·Ÿäºæ´²å„åœ‹äººäº¤å¾€\\n\\nä½ å°±æœƒç™¼ç¾\\n\\né›–ç„¶ä¸€ç›´å–Šå°æ—¥å‹å¥½ ä½†æ˜¯é‚£æ›´å¤šçš„åªæ˜¯å°ç£äººçš„ä¸€å»‚æƒ…é¡˜\\n\\n\\nå…¶å¯¦éŸ“åœ‹äººå€‹æ€§è·Ÿå°ç£äººæ˜¯æ¯”è¼ƒåƒçš„\\néŸ“åœ‹äººæ˜¯çœŸçš„æŠŠä½ ç•¶å…„å¼Ÿé‚£ç¨®\\n\\n\\n-----\\nSent from JPTT on my Xiaomi 2201117SY.\\n\\n--\\nâ€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc), ä¾†è‡ª: 49.217.62.212 (è‡ºç£)\\nâ€» æ–‡ç« ç¶²å€:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389867.A.7B7.html'},\n",
       " {'title': '[å•å¦] ä¸­è¯æ°‘åœ‹è‡ºç£é‚£éº¼æ£’æ€æ²’æœ‹å‹ï¼Ÿ',\n",
       "  'url': 'https://www.ptt.cc/bbs/Gossiping/M.1750389868.A.D73.html',\n",
       "  'content': 'å¦‚é¡Œå•¦\\nä¸­è¯æ°‘åœ‹è‡ºç£\\næœ‰å°ç©é›»\\næœ‰è¶…å¥½åƒçš„å°åƒ\\næœ‰è¶…å–„è‰¯çš„äºº\\nè¶…åå…±\\næ€éº¼æ²’æœ‰åœ‹å®¶\\nè¦è·Ÿæˆ‘å€‘åšæœ‹å‹\\nåªæœƒè¬›å‹å¥½å ´é¢è©±\\næœ‰æ²’æœ‰å¦\\nQQ\\n\\n--\\nâ€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc), ä¾†è‡ª: 49.218.209.251 (è‡ºç£)\\nâ€» æ–‡ç« ç¶²å€:\\nhttps://www.ptt.cc/bbs/Gossiping/M.1750389868.A.D73.html'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae0d22",
   "metadata": {},
   "source": [
    "### PPT + å¤šçœ‹æ¿ + å…§å®¹ (lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32762704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== é–‹å§‹æŠ“å–çœ‹æ¿ï¼šGossiping ====\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: [å•å¦] 30ã€40æ­²æ²’å­˜æ¬¾çš„æ˜¯èŠ±å»å“ªï¼Ÿï¼Ÿ\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: Re: [å•å¦] åˆ°åº•ç‚ºä»€éº¼ä¸å°ä¸­å…±é–‹ç«ï¼Ÿï¼Ÿï¼Ÿ\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: Re: [æ–°è] æ­»å›šé„­æ­¦æ¾åˆ¤æ­»å®šè®20å¹´ è²è«‹å†å¯©ä¸¦åœæ­¢\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: [å•å¦] æœˆè–ª48Kç®¡ç†è·åœ¨é«˜é›„PRå¤§æ¦‚å¤šå°‘ï¼Ÿ\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: [å•å¦] ç§»å·¥å¦¹å­éƒ½ç”¨åŒç‰Œæ´—è¡£ç²¾æ´—é«®ç²¾æ²æµ´ä¹³?\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "==== å®Œæˆçœ‹æ¿ï¼šGossiping ====\n",
      "\n",
      "==== é–‹å§‹æŠ“å–çœ‹æ¿ï¼šBaseball ====\n",
      "[Baseball] æ–‡ç« æ¨™é¡Œ: [æ–°è] MLBï¼ç´…è¥ªCampbellé­ä¸‹æ”¾3Aï¼å‰›ç²17.7å„„\n",
      "[Baseball] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Baseball] æ–‡ç« æ¨™é¡Œ: Re: [è¨è«–] æ˜¨æ—¥å¯Œé‚¦è·‘å£˜\n",
      "[Baseball] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Baseball] æ–‡ç« æ¨™é¡Œ: [åˆ†äº«] å±±æœ¬ç”±ä¼¸ç¬¬ä¸‰å±€11çƒ3K\n",
      "[Baseball] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Baseball] æ–‡ç« æ¨™é¡Œ: [æ–°è] æ‚å°‡ä¸‹åŠå­£ä¸»é¡Œæ—¥ä¾†äº†ï¼é¦–åº¦è¯åæ—¥è·é—œ\n",
      "[Baseball] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Baseball] æ–‡ç« æ¨™é¡Œ: [æ–°è] äº¤æ˜“å¾Œé¦–åº¦å°æ±º ç´…è¥ªæ•™é ­ï¼šå¾·å¼—æ–¯æœƒæƒ³æ‰“çˆ†\n",
      "[Baseball] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "==== å®Œæˆçœ‹æ¿ï¼šBaseball ====\n",
      "\n",
      "==== é–‹å§‹æŠ“å–çœ‹æ¿ï¼šBeauty ====\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] Cospaly 2044 éŸ“åœ‹ å¥³å·« å¦®å§¬\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] ä¸‰æ©‹ãã‚“\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] æœ‰å¹¾é¡†ä¿é½¡çƒ\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] æ—èŠ·èŠ¸\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] æ­£åˆèº«æå¥½\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "==== å®Œæˆçœ‹æ¿ï¼šBeauty ====\n",
      "\n",
      "ç¸½èŠ±è²»æ™‚é–“: 11.10 ç§’\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "BASE_URL = \"https://www.ptt.cc\"\n",
    "BOARDS = {\n",
    "    \"Gossiping\": \"/bbs/Gossiping/index.html\",\n",
    "    \"Baseball\": \"/bbs/Baseball/index.html\",\n",
    "    \"Beauty\": \"/bbs/Beauty/index.html\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "results_lock = threading.Lock()\n",
    "board_lock = threading.Lock()  # ç”¨ä¾†ä¿è­·çœ‹æ¿ç´šåˆ¥çš„åŸ·è¡Œæµç¨‹\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        main_content = soup.find(id=\"main-content\")\n",
    "        for tag in main_content.find_all(['div', 'span', 'a'], class_=['article-metaline', 'article-metaline-right', 'push']):\n",
    "            tag.decompose()\n",
    "        text = main_content.get_text(strip=True, separator=\"\\n\")\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def fetch_board_articles(board_name, board_path):\n",
    "    print(f\"\\n==== é–‹å§‹æŠ“å–çœ‹æ¿ï¼š{board_name} ====\")\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    url = BASE_URL + board_path\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"ç„¡æ³•å–å¾—çœ‹æ¿é é¢ {board_name}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = soup.select(\".title a\")\n",
    "\n",
    "    for a in articles[:5]:  # å…ˆæŠ“5ç¯‡ç¤ºç¯„\n",
    "        title = a.text\n",
    "        href = a['href']\n",
    "        article_url = BASE_URL + href\n",
    "        print(f\"[{board_name}] æ–‡ç« æ¨™é¡Œ: {title}\")\n",
    "\n",
    "        content = None\n",
    "        def thread_job():\n",
    "            nonlocal content\n",
    "            content = fetch_article_content(article_url)\n",
    "\n",
    "        # ä¸€ç¯‡æ–‡ç« ç”¨ä¸€å€‹åŸ·è¡Œç·’æŠ“å–æ–‡ç« å…§å®¹\n",
    "        t = threading.Thread(target=thread_job)\n",
    "        t.start()\n",
    "        t.join()  # ç­‰è©²ç¯‡æ–‡ç« æŠ“å®Œæ‰ç¹¼çºŒä¸‹ä¸€ç¯‡\n",
    "\n",
    "        print(f\"[{board_name}] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\")\n",
    "\n",
    "        with results_lock:\n",
    "            results.append({\n",
    "                \"board\": board_name,\n",
    "                \"title\": title,\n",
    "                \"url\": article_url,\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "    print(f\"==== å®Œæˆçœ‹æ¿ï¼š{board_name} ====\")\n",
    "\n",
    "def main():\n",
    "    for board_name, board_path in BOARDS.items():\n",
    "        # ç”¨board_lockç¢ºä¿ä¸€æ¬¡åªèƒ½ä¸€å€‹çœ‹æ¿åœ¨æŠ“\n",
    "        with board_lock:\n",
    "            fetch_board_articles(board_name, board_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    print(f\"\\nç¸½èŠ±è²»æ™‚é–“: {time.time() - start_time:.2f} ç§’\")\n",
    "\n",
    "    # with open(\"ptt_multiple_boards.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # print(\"è³‡æ–™å·²å­˜æˆ ptt_multiple_boards.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edace2fa",
   "metadata": {},
   "source": [
    "### PPT + çœ‹æ¿ + å…§å®¹ (å–®åŸ·è¡Œç·’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab89faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== é–‹å§‹æŠ“å–çœ‹æ¿ï¼šGossiping ====\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: [å•å¦] è¬›åˆ°BMIç‚ºä»€éº¼å°±æœƒæœ‰äººç”Ÿæ°£ï¼Ÿ\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: [å•å¦] ç‚ºä»€éº¼éŠ€æ¨“éƒ½å–œæ­¡æ”¶ç¾é‡‘å•Šï¼Ÿ\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: [å•å¦] windows ä»‹é¢æ€éº¼é€™éº¼é†œ?\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: Re: [å•å¦] éŸ“åœ‹æ­å·´ 1992å¹´æ‰è·Ÿä¸­è¯æ°‘åœ‹æ–·äº¤ï¼Ÿ\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Gossiping] æ–‡ç« æ¨™é¡Œ: [å•å¦] ä¸­è¯æ°‘åœ‹è‡ºç£é‚£éº¼æ£’æ€æ²’æœ‹å‹ï¼Ÿ\n",
      "[Gossiping] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "==== å®Œæˆçœ‹æ¿ï¼šGossiping ====\n",
      "\n",
      "==== é–‹å§‹æŠ“å–çœ‹æ¿ï¼šBaseball ====\n",
      "[Baseball] æ–‡ç« æ¨™é¡Œ: [æ–°è] MLBï¼ç´…è¥ªCampbellé­ä¸‹æ”¾3Aï¼å‰›ç²17.7å„„\n",
      "[Baseball] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Baseball] æ–‡ç« æ¨™é¡Œ: [å…¬å‘Š] æ¿ä¸»å¾µé¸é–‹å§‹\n",
      "[Baseball] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Baseball] æ–‡ç« æ¨™é¡Œ: [å…¬å‘Š] æ¿è¦ v7.0\n",
      "[Baseball] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Baseball] æ–‡ç« æ¨™é¡Œ: [æ•´ç†] 2025 æ£’çƒè³½äº‹ è½‰æ’­æ™‚é–“è¡¨\n",
      "[Baseball] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "==== å®Œæˆçœ‹æ¿ï¼šBaseball ====\n",
      "\n",
      "==== é–‹å§‹æŠ“å–çœ‹æ¿ï¼šBeauty ====\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] Cospaly 2044 éŸ“åœ‹ å¥³å·« å¦®å§¬\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] ä¸‰æ©‹ãã‚“\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] æœ‰å¹¾é¡†ä¿é½¡çƒ\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] æ—èŠ·èŠ¸\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "[Beauty] æ–‡ç« æ¨™é¡Œ: [æ­£å¦¹] æ­£åˆèº«æå¥½\n",
      "[Beauty] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\n",
      "==== å®Œæˆçœ‹æ¿ï¼šBeauty ====\n",
      "\n",
      "ç¸½èŠ±è²»æ™‚é–“: 10.58 ç§’\n",
      "è³‡æ–™å·²å­˜æˆ ptt_single_thread.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "BASE_URL = \"https://www.ptt.cc\"\n",
    "BOARDS = {\n",
    "    \"Gossiping\": \"/bbs/Gossiping/index.html\",\n",
    "    \"Baseball\": \"/bbs/Baseball/index.html\",\n",
    "    \"Beauty\": \"/bbs/Beauty/index.html\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        main_content = soup.find(id=\"main-content\")\n",
    "        for tag in main_content.find_all(['div', 'span', 'a'], class_=['article-metaline', 'article-metaline-right', 'push']):\n",
    "            tag.decompose()\n",
    "        text = main_content.get_text(strip=True, separator=\"\\n\")\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def fetch_board_articles(board_name, board_path):\n",
    "    print(f\"\\n==== é–‹å§‹æŠ“å–çœ‹æ¿ï¼š{board_name} ====\")\n",
    "    session = requests.Session()\n",
    "    session.cookies.set(\"over18\", \"1\")\n",
    "    url = BASE_URL + board_path\n",
    "    res = session.get(url, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"ç„¡æ³•å–å¾—çœ‹æ¿é é¢ {board_name}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = soup.select(\".title a\")\n",
    "\n",
    "    for a in articles[:5]:  # å…ˆæŠ“5ç¯‡ç¤ºç¯„\n",
    "        title = a.text\n",
    "        href = a['href']\n",
    "        article_url = BASE_URL + href\n",
    "        print(f\"[{board_name}] æ–‡ç« æ¨™é¡Œ: {title}\")\n",
    "\n",
    "        # å–®åŸ·è¡Œç·’ç›´æ¥å‘¼å«\n",
    "        content = fetch_article_content(article_url)\n",
    "        print(f\"[{board_name}] æ–‡ç« å…§å®¹æŠ“å–å®Œæˆ\")\n",
    "\n",
    "        results.append({\n",
    "            \"board\": board_name,\n",
    "            \"title\": title,\n",
    "            \"url\": article_url,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    print(f\"==== å®Œæˆçœ‹æ¿ï¼š{board_name} ====\")\n",
    "\n",
    "def main():\n",
    "    for board_name, board_path in BOARDS.items():\n",
    "        fetch_board_articles(board_name, board_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    print(f\"\\nç¸½èŠ±è²»æ™‚é–“: {time.time() - start_time:.2f} ç§’\")\n",
    "\n",
    "    # with open(\"ptt_single_thread.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"è³‡æ–™å·²å­˜æˆ ptt_single_thread.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8abaf9",
   "metadata": {},
   "source": [
    "## è³‡æ–™åº«\n",
    "\n",
    "- Python çš„ threading æ¨¡çµ„ç‰¹åˆ¥é©ç”¨æ–¼ I/O-bound å·¥ä½œï¼Œåƒæ˜¯ï¼š\n",
    "    - ç¶²è·¯çˆ¬èŸ²ï¼ˆç­‰å¾…ç¶²é å›æ‡‰ï¼‰\n",
    "    - æª”æ¡ˆè®€å¯«\n",
    "    - è³‡æ–™åº«æ“ä½œ\n",
    "    - ç­‰å¾…ç¡¬ç¢Ÿã€ç¶²è·¯ã€API ç­‰å¤–éƒ¨è¨­å‚™\n",
    "    - é€™äº›å·¥ä½œæœ¬èº«ä¸æœƒåƒå¤ªå¤š CPUï¼Œä½†æœƒè®“ä¸»ç¨‹å¼ã€Œå¡ä½ç­‰å¾…ã€ï¼Œé€™æ™‚ç”¨å¤šåŸ·è¡Œç·’å¯ä»¥ã€Œä¸€é‚Šç­‰ï¼Œä¸€é‚Šåšåˆ¥çš„äº‹æƒ…ã€ï¼Œå¤§å¹…æ¸›å°‘ç¸½æ™‚é–“ã€‚\n",
    "- é¿å…è¨˜æ†¶é«” Crashï¼Ÿä¸æ˜¯ä¸»è¦ç›®çš„ï¼Œä½†æœ‰é—œä¿‚\n",
    "    - å¤šåŸ·è¡Œç·’æœ¬èº«ä¸¦ä¸æœƒç‰¹åˆ¥ç¯€çœè¨˜æ†¶é«”ï¼Œåè€Œæœƒå› ç‚ºå¤šå€‹åŸ·è¡Œç·’åŒæ™‚å­˜åœ¨ï¼š\n",
    "        - ä½”ç”¨æ›´å¤šè¨˜æ†¶é«”ï¼ˆæ¯å€‹ thread æœ‰è‡ªå·±çš„å †ç–Šï¼‰\n",
    "        - è‹¥æ²’ç®¡æ§å¥½ï¼Œå¯èƒ½æœƒå°è‡´è³‡æºçˆ­å¥ªã€æ­»é–ã€è¨˜æ†¶é«”å•é¡Œ\n",
    "> ğŸ§¯ä½†ä½ æåˆ°ã€Œé¿å…è¨˜æ†¶é«” crashã€é€™ä»¶äº‹ï¼Œæ˜¯åœ¨æœ‰å¤§é‡ä»»å‹™æˆ–ä¸ç•¶åŒæ­¥æ™‚ï¼Œæœ‰äº›äººæœƒ**ä½¿ç”¨å¤šåŸ·è¡Œç·’ + é™æµï¼ˆåƒæ˜¯ Semaphoreï¼‰**ä¾†é¿å…é–‹å¤ªå¤šè³‡æºå°è‡´å´©æ½°ï¼Œé€™æ˜¯æ§åˆ¶é¢¨éšªï¼Œä¸æ˜¯æœ¬è³ªç›®çš„ã€‚\n",
    "\n",
    "| ç›®çš„                | æ˜¯å¦ä¸»è¦ç›®çš„ | è£œå……èªªæ˜                        |\n",
    "| ----------------- | ------ | --------------------------- |\n",
    "| âœ… ç¯€çœç¸½åŸ·è¡Œæ™‚é–“         | âœ… æ˜¯    | ç‰¹åˆ¥å° I/O-bound ä»»å‹™            |\n",
    "| âš ï¸ è³‡æºåˆ†é…é¿å… crash   | âŒ å¦    | ä¸æ˜¯æœ¬æ„ï¼Œä½†å¤šåŸ·è¡Œç·’å¯æ­é…æ§åˆ¶æ©Ÿåˆ¶ä¾†é¿å…        |\n",
    "| âŒ åŠ é€Ÿ CPU-heavy å·¥ä½œ | âŒ å¦    | æ‡‰è©²ç”¨å¤šè™•ç†ç¨‹åºï¼ˆ`multiprocessing`ï¼‰ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c39fda",
   "metadata": {},
   "source": [
    "### åŒæ™‚é€£ä¸‰å€‹è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f207de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ‰€æœ‰æŸ¥è©¢å®Œæˆï¼Œè€—æ™‚ï¼š3.77 ç§’\n",
      "sponsor_view: 2261 rows\n",
      "ana_table: 7974 rows\n",
      "orders: 8225 rows\n",
      "order_lines: 8094 rows\n",
      "league_map: 10342 rows\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "def query_table(table_name):\n",
    "    try:\n",
    "        conn = pymysql.connect(\n",
    "            host=\"HOSTå±è”½\",           # åŠ ä¸Šä½ çš„ DB ä¸»æ©Ÿ IP æˆ–åç¨±\n",
    "            user=\"eds\",\n",
    "            password=\"!2018Eds\",       # åŠ ä¸Šå¯†ç¢¼\n",
    "            database=\"sports_unify_db\",\n",
    "            charset=\"utf8mb4\",\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "        results = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "        return table_name, df\n",
    "    except Exception as e:\n",
    "        print(f\"[DataReader] Failed to read table `{table_name}`: {e}\")\n",
    "        return table_name, None\n",
    "    finally:\n",
    "        if \"conn\" in locals():\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def query_multiple_tables_parallel(table_names):\n",
    "    all_results = {}\n",
    "\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:  # æœ€å¤šåŒæ™‚æŸ¥è©¢ 5 å¼µè¡¨\n",
    "        futures = [executor.submit(query_table, table) for table in table_names]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            table_name, df = future.result()\n",
    "            all_results[table_name] = df\n",
    "\n",
    "    print(f\"\\næ‰€æœ‰æŸ¥è©¢å®Œæˆï¼Œè€—æ™‚ï¼š{time.time() - start_time:.2f} ç§’\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables = [\"ana_table\", \"sponsor_view\", \"order_lines\", \"orders\", \"league_map\"]#, \"match_map\", \"team_map\"]  # è‡ªè¡Œæ›¿æ›æˆä½ æœ‰çš„è¡¨\n",
    "    # tables = [\"ana_table\", \"sponsor_view\", \"order_lines\", \"match_map\", \"team_map\"]\n",
    "    results = query_multiple_tables_parallel(tables)\n",
    "\n",
    "    # ç¯„ä¾‹ï¼šé¡¯ç¤ºæ¯å¼µè¡¨çš„ç­†æ•¸\n",
    "    for table, df in results.items():\n",
    "        if df is not None:\n",
    "            print(f\"{table}: {len(df)} rows\")\n",
    "        else:\n",
    "            print(f\"{table}: âŒ æŸ¥è©¢å¤±æ•—\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeed604",
   "metadata": {},
   "source": [
    "### åŒæ™‚é€£ä¸‰å€‹è¡¨ (è¨­å®šåŒ¯å…¥æ¬¡æ•¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771290f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ana_table: æ­£åœ¨æŸ¥è©¢ç¬¬ 0 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” sponsor_view: æ­£åœ¨æŸ¥è©¢ç¬¬ 0 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” order_lines: æ­£åœ¨æŸ¥è©¢ç¬¬ 0 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” orders: æ­£åœ¨æŸ¥è©¢ç¬¬ 0 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” league_map: æ­£åœ¨æŸ¥è©¢ç¬¬ 0 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” sponsor_view: æ­£åœ¨æŸ¥è©¢ç¬¬ 12000 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” ana_table: æ­£åœ¨æŸ¥è©¢ç¬¬ 12000 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” orders: æ­£åœ¨æŸ¥è©¢ç¬¬ 12000 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” order_lines: æ­£åœ¨æŸ¥è©¢ç¬¬ 12000 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” league_map: æ­£åœ¨æŸ¥è©¢ç¬¬ 12000 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "\n",
      "æ‰€æœ‰æŸ¥è©¢å®Œæˆï¼Œç¸½è€—æ™‚ï¼š4.08 ç§’\n",
      "sponsor_view: 2261 rows\n",
      "ana_table: 7974 rows\n",
      "orders: 8225 rows\n",
      "order_lines: 8094 rows\n",
      "league_map: 10342 rows\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# å»ºç«‹é€£ç·šæ± å¼•æ“\n",
    "engine = create_engine(\"mysql+pymysql://å¯†ç¢¼å±è”½@HOSTå±è”½/sports_unify_db\", pool_size=100)# æœ€å¤§æ˜¯500\n",
    "\n",
    "def paginated_query(table_name, batch_size=12000):\n",
    "    offset = 0\n",
    "    df_list = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            sql = f\"SELECT * FROM {table_name} LIMIT {batch_size} OFFSET {offset}\"\n",
    "            print(f\"ğŸ” {table_name}: æ­£åœ¨æŸ¥è©¢ç¬¬ {offset} ç­†é–‹å§‹çš„è³‡æ–™...\")\n",
    "            try:\n",
    "                df = pd.read_sql(sql, engine)\n",
    "            except Exception as e:\n",
    "                print(f\"[æŸ¥è©¢éŒ¯èª¤] {table_name} offset {offset}ï¼š{e}\")\n",
    "                break\n",
    "\n",
    "            if df.empty:\n",
    "                break\n",
    "\n",
    "            df_list.append(df)\n",
    "            offset += batch_size\n",
    "\n",
    "        if df_list:\n",
    "            final_df = pd.concat(df_list, ignore_index=True)\n",
    "            return table_name, final_df\n",
    "        else:\n",
    "            return table_name, pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[è‡´å‘½éŒ¯èª¤] {table_name} æŸ¥è©¢å¤±æ•—ï¼š{e}\")\n",
    "        return table_name, None\n",
    "    \n",
    "def query_multiple_tables_parallel(table_names, max_workers=4):\n",
    "    all_results = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(paginated_query, table) for table in table_names]\n",
    "        for future in as_completed(futures):\n",
    "            table_name, df = future.result()\n",
    "            all_results[table_name] = df\n",
    "\n",
    "\n",
    "    print(f\"\\næ‰€æœ‰æŸ¥è©¢å®Œæˆï¼Œç¸½è€—æ™‚ï¼š{time.time() - start_time:.2f} ç§’\")\n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables = [\"ana_table\", \"sponsor_view\", \"order_lines\", \"orders\", \"league_map\"]\n",
    "    max_thread_workers = 6  # å¯åœ¨é€™è£¡è‡ªç”±èª¿æ•´ worker æ•¸\n",
    "    results = query_multiple_tables_parallel(tables, max_workers=max_thread_workers)\n",
    "\n",
    "    for table, df in results.items():\n",
    "        if df is not None:\n",
    "            print(f\"{table}: {len(df)} rows\")\n",
    "        else:\n",
    "            print(f\"{table}: æŸ¥è©¢å¤±æ•—\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ana_table: æ­£åœ¨æŸ¥è©¢ç¬¬ 0 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” sponsor_view: æ­£åœ¨æŸ¥è©¢ç¬¬ 0 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” order_lines: æ­£åœ¨æŸ¥è©¢ç¬¬ 0 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” orders: æ­£åœ¨æŸ¥è©¢ç¬¬ 0 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” sponsor_view: æ­£åœ¨æŸ¥è©¢ç¬¬ 12000 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” ana_table: æ­£åœ¨æŸ¥è©¢ç¬¬ 12000 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” orders: æ­£åœ¨æŸ¥è©¢ç¬¬ 12000 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "ğŸ” order_lines: æ­£åœ¨æŸ¥è©¢ç¬¬ 12000 ç­†é–‹å§‹çš„è³‡æ–™...\n",
      "\n",
      "æ‰€æœ‰æŸ¥è©¢å®Œæˆï¼Œç¸½è€—æ™‚ï¼š3.48 ç§’\n",
      "sponsor_view: 2261 rows\n",
      "ana_table: 7974 rows\n",
      "orders: 8225 rows\n",
      "order_lines: 8094 rows\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# å»ºç«‹é€£ç·šæ± å¼•æ“\n",
    "engine = create_engine(\"mysql+pymysql://å¯†ç¢¼å±è”½@HOSTå±è”½/sports_unify_db\", pool_size=100)  # æœ€å¤§æ˜¯500\n",
    "\n",
    "def paginated_query(table_name, batch_size=12000):\n",
    "    offset = 0\n",
    "    dfs = []  # ç”¨ list æ”¶é›†ï¼Œé¿å…åè¦†appendé€ æˆæ•ˆèƒ½å•é¡Œ\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            sql = f\"SELECT * FROM {table_name} LIMIT {batch_size} OFFSET {offset}\"\n",
    "            print(f\"ğŸ” {table_name}: æ­£åœ¨æŸ¥è©¢ç¬¬ {offset} ç­†é–‹å§‹çš„è³‡æ–™...\")\n",
    "            try:\n",
    "                df = pd.read_sql(sql, engine)\n",
    "            except Exception as e:\n",
    "                print(f\"[æŸ¥è©¢éŒ¯èª¤] {table_name} offset {offset}ï¼š{e}\")\n",
    "                break\n",
    "\n",
    "            if df.empty:\n",
    "                break\n",
    "\n",
    "            dfs.append(df)\n",
    "            offset += batch_size\n",
    "\n",
    "        if dfs:\n",
    "            final_df = pd.concat(dfs, ignore_index=True)\n",
    "            return table_name, final_df\n",
    "        else:\n",
    "            return table_name, pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[è‡´å‘½éŒ¯èª¤] {table_name} æŸ¥è©¢å¤±æ•—ï¼š{e}\")\n",
    "        return table_name, None\n",
    "\n",
    "def query_multiple_tables_parallel(table_names, max_workers=4):\n",
    "    all_results = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(paginated_query, table) for table in table_names]\n",
    "        for future in as_completed(futures):\n",
    "            table_name, df = future.result()\n",
    "            all_results[table_name] = df\n",
    "\n",
    "    print(f\"\\næ‰€æœ‰æŸ¥è©¢å®Œæˆï¼Œç¸½è€—æ™‚ï¼š{time.time() - start_time:.2f} ç§’\")\n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables = [\"ana_table\", \"sponsor_view\", \"order_lines\", \"orders\"]#, \"league_map\", \"match_map\", \"team_map\"]\n",
    "    max_thread_workers = 6\n",
    "    results = query_multiple_tables_parallel(tables, max_workers=max_thread_workers)\n",
    "\n",
    "    for table, df in results.items():\n",
    "        if df is not None:\n",
    "            print(f\"{table}: {len(df)} rows\")\n",
    "        else:\n",
    "            print(f\"{table}: æŸ¥è©¢å¤±æ•—\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
